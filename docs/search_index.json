[["index.html", "Chapter 1 Introduction", " STA4026S – Honours Analytics Section B: Theory and Application of Supervised Learning Stefan S. Britz August 2023 Chapter 1 Introduction Welcome to an introduction to supervised learning! In this set of course notes we will: cover some of the fundamental theoretical principles underpinning supervised statistical and machine learning; explore various models, algorithms, and heuristics to analyse different types of data, both for regression (continuous target variable) and classification (categorical target variable) problems; and apply these methods in R. The aim is to find a balance between breadth of topics, depth of theory, and practical application. Since we will be covering several topics in a relatively short time, the application component will focus largely on the current best practices for implementation in R. Therefore, we will mostly be using existing R packages and will not spend time coding these algorithms from scratch, with one exception in Chapter 6. The fields of statistical learning/AI/machine learning/data science/analytics/data mining/deep learning/[insert new buzzword here] are constantly evolving at a rapid pace. Although the core theory and methodology will (should) always be relevant, adaptations to the methods are regularly being developed, along with more efficient and convenient packages for implementation. Therefore, although these notes attempt to introduce you to up-to-date modern frameworks, note that these things change over time. Also note that this is by no means an exhaustive exploration of either theory, methods, or application, but it will imbue you with a skill set with which to tackle various problems and provide a solid foundation for further learning. These notes draw from various sources, with the theoretical aspects largely relying on An Introduction to Statistical Learning with Applications in R (James et al., 2013) and Elements of Statistical Learning (Hastie et al., 2009), both of which are freely available here and here, respectively. It is recommended that you keep the former on hand, as you will be referred to sections therein for reading. Other sources will be referenced as and when they are used. Happy learning! "],["supervised-learning.html", "Chapter 2 Supervised Learning 2.1 Bias-Variance trade-off 2.2 Model validation 2.3 Side note: Statistical learning vs machine learning 2.4 Homework exercises", " Chapter 2 Supervised Learning This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, some of which you should be familiar with already. Whereas there are many good sources that provide a more comprehensive discussion, Chapter 2 of James et al. (2013) is sufficient for the level and scope of this course. Using a set of observations to uncover some underlying process in the real world is the basic premise of “learning from data” (Abu-Mostafa et al., 2012, p. 11). By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course. The basic distinction is that supervised learning refers to cases where there is some target variable, usually indicated as \\(Y\\), whereas if we are interested in the structures and patterns in explanatory variables1 only, we refer to unsupervised learning. Given a quantitative response \\(Y\\) and a set of \\(p\\) predictors \\(X_1, X_2, \\ldots, X_p\\), we are interested in the assumed, unobserved function that maps the inputs to the outputs: \\[Y = \\underbrace{f(X)}_{systematic} + \\underbrace{\\epsilon}_{random},\\] where \\(f(.)\\) represents the fixed, but unknown function and \\(\\epsilon\\) is a random error term, independent of \\(X\\), with \\(E(\\epsilon) = 0\\). By estimating \\(f\\) such that \\[\\hat{Y} = \\hat{f}(X),\\] we allow for both prediction of \\(Y\\) – which is the primary goal in forecasting – and inference, i.e. describing how \\(Y\\) is affected by changes in \\(X\\). Hypothesising \\(\\hat{f}\\) can be done in two ways, namely via a parametric or a non-parametric approach. Parametric approach Here an assumption is made about the the functional form of \\(f\\), for example the linear model \\[f(\\boldsymbol{X}) = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j.\\] The best estimate of \\(f\\) is now defined as the set of parameters \\(\\hat{\\boldsymbol{\\beta}}\\) that minimise some specified loss function. Given a set of observations \\(\\mathcal{D}=\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^n\\) – henceforth referred to as the training set – we could use ordinary least squares to minimise the mean squared error (MSE): \\[MSE = \\frac{1}{n}\\sum_{i=1}^n\\left[y_i - \\hat{f}(x_i)\\right]^2 \\] Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters. Non-parametric approach Another option is to make no explicit assumptions regarding the functional form of \\(f\\). This allows one to fit a wide range of possible forms for \\(f\\) – in these notes we consider K-nearest neighbours and tree-based methods – but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation. The objective remains to find \\(f\\) that fits the available data as closely as possible, whilst avoiding overfitting to ensure that the model generalises well to unseen data. Generalisation The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model, referred to as out-of-sample observations. Consider the case where the estimated function \\(\\hat{f}\\) is fixed and out-of-sample observations of the variables are introduced, which we will denote as \\(\\{\\boldsymbol{x}_0, y_0\\}\\). The expected MSE for these test set observations (see Section 2.2) can be deconstructed as follows: \\[\\begin{align} E\\left[y_0 - \\hat{f}(\\boldsymbol{x}_0) \\right]^2 &amp;= E\\left[f(\\boldsymbol{x}_0) + \\epsilon - \\hat{f}(\\boldsymbol{x}_0)\\right]^2 \\\\ &amp;= E\\left[\\left(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right)^2 + 2\\epsilon \\left(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0) \\right) + \\epsilon^2\\right] \\\\ &amp;= E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right]^2 + 2E[\\epsilon]E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right] + E\\left[\\epsilon^2\\right] \\\\ &amp;= \\underbrace{E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right]^2}_{reducible} + \\underbrace{Var(\\epsilon)}_{irreducible} \\tag{2.1} \\end{align}\\] The primary goal of machine learning is to find an \\(\\hat{f}\\) that best approximates the underlying, unknown relationship between the input and output variables by minimising the reducible error. Note that because of the irreducible component (the “noise” in the data), there will always be some lower bound for the theoretical MSE, and that this bound is almost always unknown in practice (James et al., 2013, p. 19). By now you will be familiar with the concept of bias-variance trade-off, according to which we attempt to find a sufficiently (but not overly) complex model. The reducible error component can be decomposed further to help illustrate this trade-off. 2.1 Bias-Variance trade-off Consider again a fixed \\(\\hat{f}\\) and out-of-sample observations \\(\\{\\boldsymbol{x}_0, y_0\\}\\). For ease of notation, let \\(f = f(\\boldsymbol{x}_0)\\) and \\(\\hat{f} = \\hat{f}(\\boldsymbol{x}_0)\\). Also note that \\(f\\) is deterministic such that \\(E\\left[f\\right] = f\\). Starting with the reducible error in Equation (2.1), we have \\[\\begin{align} E\\left[f - \\hat{f} \\right]^2 &amp;= E\\left[\\hat{f} - f \\right]^2 \\\\ &amp;= E\\left[\\hat{f} - E(\\hat{f}) + E(\\hat{f}) - f \\right]^2 \\\\ &amp;= E\\left\\{ \\left[\\hat{f} - E(\\hat{f})\\right]^2 + 2\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right] + \\left[E(\\hat{f}) - f\\right]^2 \\right\\} \\\\ &amp;= E\\left[\\hat{f} - E(\\hat{f})\\right]^2 + 2E\\left\\{\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right]\\right\\} + E\\left[E(\\hat{f}) - f\\right]^2 \\\\ &amp;= Var\\left[\\hat{f}\\right] + 0 + \\left[E(\\hat{f}) - f\\right]^2 \\\\ &amp;= Var\\left[\\hat{f}\\right] + Bias^2\\left[\\hat{f}\\right] \\tag{2.2} \\end{align}\\] Showing that the crossproduct term equals zero: \\[\\begin{align} E\\left\\{\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right]\\right\\} &amp;= E\\left[\\hat{f}E(\\hat{f}) - E(\\hat{f})E(\\hat{f}) - \\hat{f}f + E(\\hat{f})f\\right] \\\\ &amp;= E(\\hat{f})E(\\hat{f}) - E(\\hat{f})E(\\hat{f}) - E(\\hat{f})f + E(\\hat{f})f \\\\ &amp;= 0 \\end{align}\\] Therefore, in order to minimise the expected test MSE we need to find a model that has the lowest combined variance and (squared) bias. The variance represents the extent to which \\(\\hat{f}\\) changes between different training samples taken from the same population. The bias of \\(\\hat{f}\\) is simply the error that is introduced by approximating the real-world relationship with a simpler representation. Note, however, that since \\(f\\) is generally unknown, the bias component cannot be directly observed or measured outside of simulations. However, these simulations may help us illustrate how the bias and variance change as model complexity increases. Although the concepts of model complexity and flexibility are not necessarily perfectly defined – depending on the class of model being hypothesised – the following example should provide an intuitive understanding. 2.1.1 Example 1 – Simulation To allow for easy visualisation, let us consider a simple function with only one feature: \\[Y = X + 2\\cos(5X) + \\epsilon,\\] where \\(\\epsilon \\sim N(0, 2)\\). Below we simulate \\(n = 100\\) observations from \\(X \\sim U(-2,2)\\), to which we fit cubic smoothing splines of increasing complexity. [Splines are beyond the scope of this course, but provide an easy-to-see illustration of “flexibility”] rm(list = ls()) set.seed(4026) #Simulated data x &lt;- runif(100, -2, 2) y &lt;- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2)) #The true function xx &lt;- seq(-2, 2, length.out = 1000) f &lt;- xx + 2*cos(5*xx) #Fit cubic splines with increasing degrees of freedom for(dof in 2:50){ fhat &lt;- smooth.spline(x, y, df = dof) plot(x, y, pch = 16) lines(xx, f, &#39;l&#39;, lwd = 2) lines(fhat, col = &#39;blue&#39;, lwd = 2) title(main = paste(&#39;Degrees of freedom:&#39;, dof)) legend(&#39;bottomright&#39;, c(&#39;f(x) - True&#39;, expression(hat(f)(x) ~ &#39;- Cubic spline&#39;)), col = c(&#39;black&#39;, &#39;blue&#39;), lty = 1, lwd = 2) } Figure 2.1: Cubic splines with varying degrees of freedom fitted to a sample of 100 datapoints drawn from \\(Y = X + 2\\cos(5X) + \\epsilon\\), with \\(\\epsilon \\sim N(0, 2)\\). This serves to illustrate how the model’s degrees of freedom is directly proportional to the model’s complexity. However, to extricate the bias and variance components, we need to observe these models’ fit on out-of-sample data across many random realisations of training samples. In the following simulation we again observe \\(n=100\\) training observations at a time, to which the same models of varying complexity as above are fitted. Each model’s fit is then assessed on a set of 100 testing observations, where the \\(\\boldsymbol{x}_0\\) (and, therefore, true \\(f(\\boldsymbol{x}_0)\\)) are fixed, but random noise is added. This process is repeated 1000 times, such that we can keep track of how each test observation’s predictions vary across the iterations, as well as the errors. set.seed(1) n &lt;- 100 #Sample size num_sims &lt;- 1000 #Number of iterations (could be parallelised) dofs &lt;- 2:25 #Model complexities var_eps &lt;- 2 #Var(epsilon): The irreducible error pred_mat &lt;- matrix(nrow = num_sims, ncol = n) #To store each set of predictions mses &lt;- vector(length = num_sims) #Also want to track the testing MSEs red_err &lt;- vector(length = num_sims) #As well as the reducible error #Herein we will capture the deconstructed components for each model results &lt;- data.frame(Var = NA, Bias2 = NA, Red_err = NA, MSE = NA) #Testing data x_test &lt;- runif(n, -2, 2) f_test &lt;- x_test + 2*cos(5*x_test) #This is the part we don&#39;t know outside sims!! d &lt;- 0 #To keep track of dof iterations, even when changing the range for(dof in dofs) { #Repeat over all model complexities d &lt;- d+1 for(iter in 1:num_sims){ #Training data x_train &lt;- runif(n, -2, 2) y_train &lt;- x_train + 2*cos(5*x_train) + rnorm(n, sd = sqrt(var_eps)) #Add the noise y_test &lt;- f_test + rnorm(n, sd = sqrt(var_eps)) #Fit cubic spline spline_mod &lt;- smooth.spline(x_train, y_train, df = dof) #Predict on OOS data yhat &lt;- predict(spline_mod, x_test)$y #And store pred_mat[iter, ] &lt;- yhat red_err[iter] &lt;- mean((f_test - yhat)^2) mses[iter] &lt;- mean((y_test - yhat)^2) } #Average each component over all iterations var_fhat &lt;- mean(apply(pred_mat, 2, var)) #E[\\hat{f} - E(\\hat{f})]^2 bias2_fhat &lt;- mean((colMeans(pred_mat) - f_test)^2) #E[E(\\hat{f}) - f]^2 reducible &lt;- mean(red_err) #E[f - \\hat{f}]^2 MSE &lt;- mean(mses) #E[y_0 - \\hat{f}]^2 results[d, ] &lt;- c(var_fhat, bias2_fhat, reducible, MSE) } #Plot the results plot(dofs, results$MSE, &#39;l&#39;, col = &#39;darkred&#39;, lwd = 2, xlab = &#39;Model complexity&#39;, ylab = &#39;&#39;, ylim = c(0, max(results))) lines(dofs, results$Bias2, &#39;l&#39;, col = &#39;lightblue&#39;, lwd = 2) lines(dofs, results$Var, &#39;l&#39;, col = &#39;orange&#39;, lwd = 2) lines(dofs, results$Red_err, &#39;l&#39;, lty = 2, lwd = 2) legend(&#39;topright&#39;, c(&#39;MSE&#39;, expression(Bias^2 ~ (hat(f))), expression(Var(hat(f))), &#39;Reducible Error&#39;), col = c(&#39;darkred&#39;, &#39;lightblue&#39;, &#39;orange&#39;, &#39;black&#39;), lty = c(rep(1, 3), 2), lwd = 2) abline(v = dofs[which.min(results$MSE)], lty = 3) #Complexity minimising MSE abline(h = var_eps, lty = 3) #MSE lower bound Figure 2.2: Averaged error components over 1000 simulations of samples of \\(n=100\\). The horizontal dashed line represent the minimum lower bound for the test MSE. The vertical dashed line indicates the point at which both the test MSE and reducible error are minimised. As a quick sanity check before interpreting this result, let us add up the components – which were calculated separately – and see whether we indeed observe that \\(E\\left[f - \\hat{f} \\right]^2 = Var\\left[\\hat{f}\\right] + Bias^2\\left[\\hat{f}\\right]\\) as per Equation (2.1) and \\(\\text{Test MSE} = E\\left[y_0 - \\hat{f}\\right]^2 = E\\left[f - \\hat{f} \\right]^2 + Var(\\epsilon)\\) as per Equation (2.2). Note that we will need to have a small tolerance for discrepancy, since we have approximated the expected values by averaging over only 1000 realisations. This approximation will become more accurate as the number of iterations is increased. #Is reducible error = var(fhat) + bias^2(fhat)? ifelse(isTRUE(all.equal(results$Red_err, results$Var + results$Bias2, tolerance = 0.001)), &#39;Happy days! :D&#39;, &#39;Haibo...&#39;) ## [1] &quot;Happy days! :D&quot; #Is Test MSE = var(fhat) + bias^2(fhat) + var(eps)? ifelse(isTRUE(all.equal(results$MSE, results$Var + results$Bias2 + var_eps, tolerance = 0.01)), &#39;Happy days! :D&#39;, &#39;Haibo...&#39;) ## [1] &quot;Happy days! :D&quot; Figure 2.2 illustrates the general error pattern when learning from data: As model complexity/flexibility increases, the variance across multiple training samples increases, whilst the (squared) bias decreases as the estimated function gets closer to the true pattern on average. Note that \\(E(\\epsilon^2) = Var(\\epsilon)\\) remains constant. This decrease in bias\\(^2\\) initially offsets the increase in variance such that the test MSE initially decreases. However, from some complexity/flexibility of \\(\\hat{f}\\), the decrease in bias\\(^2\\left(\\hat{f}\\right)\\) is offset by the increase in \\(Var\\left(\\hat{f}\\right)\\), at which point the model starts to overfit and the test MSE starts increasing. This is the bias-variance trade-off. In this particular example, we see that of all the cubic splines, one with 13 degrees of freedom most closely captures the underlying pattern in the data, as measured by the test MSE. The fundamental challenge in statistical learning is to postulate a model of the data that yields both a low bias and variance, whilst policing the model complexity such that the sum of these error components are minimised. In the above example, we knew what the underlying function was as well as the residual variance. However, when modelling data generated in some real-world environment, we do not observe \\(f\\) and therefore cannot explicitly compute the test MSE. In order to estimate the test MSE, we make use of model validation procedures. 2.2 Model validation Imagine there are two students who have been subjected to the same set of lectures, notes, and homework exercises, which you can view as their training data used to learn the true subject knowledge. When studying for the test – which is designed to test this knowledge, i.e. the test set in our analogy – they take two different approaches: Student A, a model student, tries to master the subject matter by focusing on the course material, testing themself with new homework exercises after studying some completed ones first. Student B, however, managed to obtain a copy of the test in advance through some nefarious means, and plans to prove their knowledge of the subject matter by preparing only for this specific set of questions. Even though student B’s test score will in all likelihood be better, does this mean that they have obtained and retained more knowledge? Certainly not! Suppose the lecturer catches wind of this cheating and swaps the initial test with a new set of randomised questions. Which of the two approaches would you expect to yield better results on such a test, on average? When comparing different statistical models, we would like to select the one that we think will work best on unseen test data. But if we use the test data to make this decision, this will also be cheating, and we will be no better off for it. Like student A though, we can leave out some exercises in the training data and use these to validate our learning, i.e. gauge how well we would do in the test. 2.2.1 Validation set One way to create a validation set (or hold-out set) is to just leave aside, in a randomised way, a portion of the training data, say 30%. We then train models on the other 70% of the data only, test them on the validation set, and select the model that yields the lowest validation MSE, which serves as an estimate of test set performance. Although there are some situations in which this approach is merited, it has two potential flaws: Due to the single random split, the validation estimate of the test error can be highly variable. Since we are reducing our training data, the model sees less information, generally leading to worse performance. Therefore, the validation error may overestimate the test error. We will not go into any more detail than this on the validation set approach, but rather focus on cross-validation (CV) strategies, which addresses these two issues. 2.2.2 \\(k\\)-fold CV With this approach, the training set is randomly divided into \\(k\\) groups, or folds, of (approximately) the same size. Each fold gets a turn to act as the validation set, with the model trained on the remaining \\(k-1\\) folds. Therefore, the training process is repeated \\(k\\) times, each yielding an estimate of the test error, denoted as \\(MSE_1,\\ MSE_2,\\ldots,\\ MSE_k\\). These values are then averaged over all \\(k\\) folds to yield the \\(k\\)-fold CV estimate: \\[CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^k MSE_i\\] The next obvious question is: What value of \\(k\\) should we choose? Start by considering the lowest value, \\(k=2\\). This would be the same as the validation set approach with a 50% split, except that each half of the data will get a chance to act as training and validation set. Therefore, we still expect the validation error to overestimate the test error, or in other words, there will be some bias. As we increase \\(k\\), the estimated error will become more unbiased, since each fold will allow the model to capture more of the underlying pattern. However, just as with model complexity we also need to consider the variance aspect. Consider now the other extreme, when \\(k = n\\). Here we have what is referred to as Leave-one-out cross-validation (LOOCV), since we have \\(n\\) folds, each leaving out just one observation for validation. Each of these \\(n\\) training sets will be almost identical, such that there will be very high correlation between them. Now, remember that when we add correlated random variables (note that averaging involves summation), then the correlation affects the resulting variance: \\[Var(X+Y) = \\sigma^2_X + \\sigma^2_Y + 2\\frac{\\rho_{XY}}{\\sigma_X\\sigma_Y}\\] Therefore, larger \\(k\\) implies larger variation of the estimated error. This means that the same bias-variance trade-off applies to \\(k\\)-fold CV! In practice, it has been shown that \\(k = 5\\) or \\(k = 10\\) yields a good balance such that the test error estimate does not suffer from excessively high bias nor variance. Also note that as \\(k\\) increases, the computational cost increases proportionally, since \\(k\\) separate models must be fitted to \\(k\\) different data splits. This could cause unnecessarily long training times for large datasets/complicated models, such that a smaller \\(k\\) might be preferable. To illustrate the implementation, let us return to the earlier example, where we will pretend that we do not know the underlying relationship we are trying to estimate. 2.2.3 Example 1 – Simulation (continued) Before using the CV error to determine the ideal model complexity, let us first illustrate the concept of cross-validation for a single model, say a cubic spline with 8 degrees of freedom, with \\(k = 10\\). set.seed(4026) #Simulated data x &lt;- runif(100, -2, 2) y &lt;- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2)) cv_k &lt;- c() train_err &lt;- c() #10-fold CV ind &lt;- sample(1:100) #Don&#39;t actually need to randomise here, but should in general for(k in 1:10){ valid_ind &lt;- ind[seq((1+10*(k-1)), 10*k)] fit &lt;- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8) test &lt;- predict(fit, x[valid_ind])$y train_fit &lt;- predict(fit, x[-valid_ind])$y cv_k &lt;- c(cv_k, mean((test - y[valid_ind])^2)) train_err &lt;- c(train_err, mean((train_fit - y[-valid_ind])^2)) #Should write the above into a function... #But the plotting needs to be inside the loop for the rendering par(mfrow = c(1, 2)) plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = &#39;x&#39;, ylab = &#39;y&#39;, xlim = c(min(x), max(x)), ylim = c(min(y), max(y))) points(x[valid_ind], y[valid_ind], pch = 16, col = &#39;gray&#39;) segments(x[valid_ind], y[valid_ind], x[valid_ind], test, col = &#39;red&#39;, lty = 3, lwd = 2) lines(fit, col = &#39;blue&#39;, lwd = 2) title(main = paste(&#39;Fold:&#39;, k)) legend(&#39;bottomright&#39;, c(&#39;Training&#39;, &#39;Validation&#39;, &#39;Errors&#39;), col = c(&#39;black&#39;, &#39;gray&#39;, &#39;red&#39;), pch = c(16, 16, NA), lty = c(NA, NA, 3), lwd = c(NA, NA, 2)) plot(1:k, cv_k, &#39;b&#39;, pch = 16, col = &#39;red&#39;, lwd = 2, xlab = &#39;Fold&#39;, ylab = &#39;MSE&#39;, xlim = c(1, 10), ylim = c(1, 5.5)) lines(1:k, train_err, &#39;b&#39;, pch = 16, lwd = 2) legend(&#39;topright&#39;, c(&#39;Training&#39;, &#39;Validation&#39;), col = c(&#39;black&#39;, &#39;red&#39;), lwd = 2) } Figure 2.3: Left: The training (black) and validation (grey) portions of the dataset across 10 folds, with the fitted cubic spline with 8 degrees of freedom in blue. Right: The resulting training (black) and validation (red) MSEs across 10 folds. Here we see that, as expected, the validation error is noticeably more variable than the training error across the folds. We can calculate the CV MSE as 2.77, although on its own this value is not particularly insightful. When comparing it to that of other models, though, we can determine which model complexity is estimated to yield the lowest test error. #Keep track of MSE per fold, per model fold_mses &lt;- matrix(nrow = 10, ncol = length(dofs)) for(k in 1:10){ cv_ind &lt;- ind[seq((1+10*(k-1)), 10*k)] d &lt;- 0 for(dof in dofs){ #Using the same dofs as earlier d &lt;- d + 1 fit &lt;- smooth.spline(x[-cv_ind], y[-cv_ind], df = dof) test &lt;- predict(fit, x[cv_ind])$y fold_mses[k, d] &lt;- mean((test - y[cv_ind])^2) } } #Average over all folds cv_mses &lt;- colMeans(fold_mses) # Compare the true MSE from earlier plot(dofs, results$MSE, &#39;l&#39;, col = &#39;darkred&#39;, lwd = 2, xlab = &#39;Model complexity&#39;, ylab = &#39;&#39;, ylim = c(0, max(cv_mses))) lines(dofs, cv_mses, &#39;l&#39;, col = &#39;grey&#39;, lwd = 2) legend(&#39;bottomright&#39;, c(&#39;CV MSE&#39;, &#39;True test MSE&#39;), col = c(&#39;gray&#39;, &#39;darkred&#39;), lty = 1, lwd = 2) abline(v = dofs[which.min(results$MSE)], lty = 3) points(dofs[which.min(cv_mses)], min(cv_mses), pch = 13, cex = 2.5, lwd = 2) Figure 2.4: 10-fold cross-validation error curve (grey) for cubic splines with varying degrees of freedom, with the minimum point indicated by the crossed circle. The red line indicates the true test MSE being estimated. Because we simulated these data, we know that the cubic spline yielding the lowest expected test MSE is one with 13 degrees of freedom. Applying 10-fold CV to our 100 training data points resulted in an estimated optimal model with 12 degrees of freedom. It is interesting to note that the CV error consistently overestimated the true error. This is likely due to the relatively small dataset; remember that we only tested on 10 observations per fold! The shape of the true MSE curve was captured relatively well by the CV process in this example. This section provided a succinct illustration of model validation. For a detailed discussion, see Section 5.1 of James et al. (2013). In the following chapters we will move beyond simulated data and apply these methods to various datasets as we encounter different classes of models and other techniques. Although there is much value in coding the CV procedure from scratch, it is built into various R packages, which we will leverage going forward. 2.3 Side note: Statistical learning vs machine learning It may seem that we use the terms “statistical learning” and “machine learning” interchangeably, so is there a difference? The distinction between these two concepts can sometimes be blurred with the paradigms largely overlapping, and some might argue that the difference is mostly semantic. In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance. However, the principles discussed in this chapter forms the core of both approaches – both are concerned with extracting insights from data and making predictions, although they may approach these goals with slightly different philosophical and methodological perspectives. Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. While understanding the underlying data-generating process is still important in machine learning, the focus is often more on achieving optimal predictive performance. Statistical learning approaches are also characterised by explicit assumptions about the underlying statistical distributions and relationships between variables, whereas machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions. For the purposes of our study throughout this course, these distinctions are not of consequence and we will adopt both perspectives throughout. 2.4 Homework exercises For Example 1, edit the provided code such that LOOCV is applied. Does this method suggest a different model complexity? Now do the same for 5-fold CV. What changes in the CV curve do you observe as \\(k\\) changes? Often referred to as features in the learning context, or predictors in supervised learning specifically.↩︎ "],["linear-model-selection-regularisation.html", "Chapter 3 Linear Model Selection &amp; Regularisation 3.1 Linear Regression Models 3.2 \\(L_1\\) and \\(L_2\\) regularisation 3.3 Elastic-net 3.4 Homework exercises", " Chapter 3 Linear Model Selection &amp; Regularisation In the previous chapter we discussed cross-validation (CV) as a procedure for estimating the out-of-sample performance of models of the same form, but different complexity, where each model was considered a separate hypothesised representation of the underlying function – \\(f\\) – mapping all the explanatory variables (features) to the dependent (target) variable. In the following sections we will start by fitting a linear model, with the focus then on variable selection, i.e. deciding which features to include in the model. Instead of deciding on the model “settings” beforehand – which we will in later chapters come to know as hyperparameters – we will rather adjust the fitted model parameters by means of regularisation, also referred to as shrinkage methods. Following that, we will cover dimension reduction methods. This chapter is loosely based on chapter 6 of James et al. (2013) and chapter 3 of Hastie et al. (2009) and assumes some basic knowledge of linear regression models. 3.1 Linear Regression Models Although few real-world relationships can be considered truly linear, the linear model offers some distinct advantages, most notably in the clear interpretation of features2. Furthermore, they often perform surprisingly well on a range of problems. For some real-valued output \\(Y\\) and input vector \\(\\boldsymbol{X}&#39; = [X_1, X_2, \\ldots, X_p]\\), the model is defined as: \\[\\begin{equation} Y = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j + \\epsilon, \\tag{3.1} \\end{equation}\\] where \\(\\epsilon \\sim N(0, \\sigma^2)\\). The most popular method of estimating the regression parameters based on the training set \\(\\mathcal{D}=\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^n\\), is ** ordinary least squares** (OLS), where we find the coefficients \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_p]&#39;\\) to minimise the residual sum of squares \\[\\begin{equation} RSS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2, \\tag{3.2} \\end{equation}\\] noting that this does not imply any assumptions on the validity of the model. To minimise \\(RSS(\\boldsymbol{\\beta})\\), let us first write (3.1) in matrix form: \\[\\begin{equation} _n\\boldsymbol{Y}_1 = {}_n\\boldsymbol{X}_{(p+1)} \\boldsymbol{\\beta}_1 + {}_n\\boldsymbol{\\epsilon}_1, \\tag{3.3} \\end{equation}\\] where the first column of \\(\\boldsymbol{X}\\) is \\(\\boldsymbol{1}:n\\times1\\). Now we can write \\[\\begin{equation} RSS(\\boldsymbol{\\beta}) = \\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right)&#39;\\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right), \\tag{3.4} \\end{equation}\\] which is a quadratic function in the \\(p+1\\) parameters. Differentiating with respect to yields \\[\\begin{align} \\frac{\\partial RSS}{\\partial \\boldsymbol{\\beta}} &amp;= -2\\boldsymbol{X}&#39;\\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right) \\\\ \\frac{\\partial^2 RSS}{\\partial \\boldsymbol{\\beta}\\partial \\boldsymbol{\\beta}&#39;} &amp;= 2\\boldsymbol{X}&#39;\\boldsymbol{X} \\tag{3.5} \\end{align}\\] If \\(\\boldsymbol{X}\\) is of full column rank – a reasonable assumption when \\(n \\geq p\\) – then \\(\\boldsymbol{X}&#39;\\boldsymbol{X}\\) is positive definite. We can then set the first derivative to zero to obtain the unique solution \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}&#39;\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}\\boldsymbol{y} \\tag{3.6} \\end{equation}\\] These coefficient estimates define a fitted linear regression model. The focus of this section is on methods for improving predictive accuracy, therefore we will not cover inference on the regression parameters or likelihood ratio tests here. The following section instead answers the question: “How can one simplify a regression model, either by removing covariates or limiting their contribution, in order to improve predictive performance?” Before delving into regularisation methods, we will briefly note the existence of subset selection methods. Subset selection Although subject to much criticism, there are some specific conditions in which subset selection could yield satisfactory results, for instance when \\(p\\) is small and there is little to no multicollinearity. This selection can generally be done in two ways: Best subset selection This approach identifies the best fitting model across all \\(2^p\\) combinations of predictors, by first identifying the best \\(k\\)-variable model \\(\\mathcal{M}_k\\) according to RSS, for all \\(k = 1, 2, \\ldots, p\\). Stepwise selection Starting with either the null (forward stepwise) or saturated (backward stepwise) model, predictors are sequentially added or removed respectively according to some improvement metric. One can also apply a hybrid method, which considers both adding and removing a variable at each step. Typically, either Mallow’s \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\) is used for model comparison in subset selection. Because subset selection is a discrete process, with variables either retained or discarded, it often exhibits high variance, thereby failing to reduce the test MSE. Regularisation offers a more continuous, general-purpose and usually quicker method of controlling model complexity. Note that although the linear model is used here to illustrate the theory of regularisation, it can be applied to any parametric model. 3.2 \\(L_1\\) and \\(L_2\\) regularisation As an alternative to using least squares, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularises the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero by imposing a penalty on their size. Hence, regularisation is also referred to as shrinkage methods. This approach has the effect of significantly reducing the coefficient estimates’ variance, thereby reducing the variance component of the total error. The two best-known techniques for shrinking the regression coefficients towards zero, are ridge regression and the lasso. 3.2.1 Ridge regression – \\(L_2\\) Ridge regression was initially developed as a method of dealing with highly correlated predictors in regression analysis. Instead of finding regression coefficients to minimise (3.2), the ridge coefficients minimise a penalised residual sum of squares: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_R = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}}\\left\\{ \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^p\\beta_j^2 \\right\\}. \\tag{3.7} \\end{equation}\\] The complexity parameter \\(\\lambda \\geq 0\\) controls the amount of shrinkage. As \\(\\lambda\\) increases, the coefficients are shrunk towards zero, whilst \\(\\lambda = 0\\) yield the OLS. Neural networks also implement regularisation by means of penalising the sum of the squared parameters; in this context it is referred to as weight decay. The term “\\(L_2\\) regularisation”, also stylised as \\(\\ell_2\\), arises because the regularisation penalty is based on the \\(L_2\\) norm3 of the regression coefficients. The \\(L_2\\) norm of a vector \\(\\boldsymbol{\\beta}\\) is given by \\(||\\boldsymbol{\\beta}||_2 = \\sqrt{\\sum_{i=1}^p \\beta_i^2}\\). The optimisation problem in (3.7) can also be written as \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_R = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2, \\\\ \\text{subject to } \\sum_{j=1}^p\\beta_j^2 \\leq \\tau, \\tag{3.8} \\end{equation}\\] where \\(\\tau\\), representing the explicit size constraint on the parameters, has a one-to-one correspondence with \\(\\lambda\\) in (3.7). When collinearity exists in a linear regression model the regression coefficients can exhibit high variance, such that correlated predictors, which carry similar information, can have large coefficients with opposite signs. Imposing a size constraint on the coefficients addresses this problem. It is important to note that since ridge solutions are not equivariant under scaling of the inputs4, the inputs are generally standardised before applying this method. Note also the omission of \\(\\beta_0\\) in the penalty term. Whereas the regression coefficients depend on the predictors in the model, the bias term is a constant independent of the predictors, i.e. it is a property of the data that does not change as variables are added or removed. Now, if the inputs are standardised such that each \\(x_{ij}\\) is replaced by \\(\\frac{x_{ij} - \\bar{x}_j}{s_{x_j}}\\), then \\(\\beta_0\\) is estimated by \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i\\) and \\(\\beta_1, \\ldots, \\beta_p\\) are estimated by a ridge regression without an intercept, using the centered \\(x_{ij}\\). The same applies to the lasso discussed in the following section. Assuming this centering has been done, the input matrix \\(\\boldsymbol{X}\\) then becomes \\(n\\times p\\), such that the penalised RSS, now viewed as a function of \\(\\lambda\\), can be written as \\[\\begin{equation} RSS(\\lambda) = \\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right)&#39;\\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right) + \\lambda\\boldsymbol{\\beta}&#39;\\boldsymbol{\\beta}, \\tag{3.9} \\end{equation}\\] yielding the ridge regression solution \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}&#39;\\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}\\boldsymbol{y} \\tag{3.10}, \\end{equation}\\] where \\(\\boldsymbol{I}\\) is the \\(p \\times p\\) identity matrix. Equation (3.10) shows that the ridge regression addresses singularity issues that can arise when the predictor variables are highly correlated. The regularisation term ensures that even if \\(\\boldsymbol{X}&#39;\\boldsymbol{X}\\) is singular, the modified matrix \\(\\boldsymbol{X}&#39;\\boldsymbol{X} + \\lambda\\boldsymbol{I}\\) is guaranteed to be non-singular, allowing for stable and well-defined solutions to be obtained. Although all of the above can be neatly explored via simulated examples, our focus will now move away from this controlled environment and towards using R packages to implement this methodology on some real-world data. 3.2.2 Example 2 – Prostate Cancer This dataset formed part of the now retired ElemStatLearn R package; it’s details can be found here. The goal is to model the (log) prostate-specific antigen (lpsa) for men who were about to receive a radical prostatectomy, based on eigth clinical measurements. The data only contain 97 observations, 30 of which are set aside for testing purposes. Looking at the correlation between the features, we see that all the features are positively correlated with the response variable, ranging from weak to strong correlation. We also observe some strong correlation between features, which could be of concern for a regression model. Note! \\(\\texttt{svi}\\) and \\(\\texttt{gleason}\\) are actually binary and ordinal variables respectively, but we will treat them as numeric for the sake of simplicity in this illustration. library(corrplot) #For correlation plot dat_pros &lt;- read.csv(&#39;data/prostate.csv&#39;) # Extract train and test examples and drop the indicator column train_pros &lt;- dat_pros[dat_pros$train, -10] test_pros &lt;- dat_pros[!dat_pros$train, -10] corrplot(cor(train_pros), method = &#39;number&#39;, type = &#39;upper&#39;) Figure 3.1: Correlations of all variables in the prostate cancer data Next, we standardise the predictors and fit a saturated linear model. library(kableExtra) library(broom) #For nice tables # Could do the following neatly with tidyverse, this is a MWE y &lt;- train_pros[, 9] #9th column is target variable x &lt;- train_pros[, -9] x_stand &lt;- scale(x) #standardise for comparison train_pros_stand &lt;- data.frame(x_stand, lpsa = y) # Fit lm using all features lm_full &lt;- lm(lpsa ~ ., train_pros_stand) lm_full %&gt;% tidy() %&gt;% kable(digits = 2, caption = &#39;Saturated linear model fitted to the prostate cancer dataset (features standardised)&#39;) Table 3.1: Saturated linear model fitted to the prostate cancer dataset (features standardised) term estimate std.error statistic p.value (Intercept) 2.45 0.09 28.18 0.00 lcavol 0.72 0.13 5.37 0.00 lweight 0.29 0.11 2.75 0.01 age -0.14 0.10 -1.40 0.17 lbph 0.21 0.10 2.06 0.04 svi 0.31 0.13 2.47 0.02 lcp -0.29 0.15 -1.87 0.07 gleason -0.02 0.14 -0.15 0.88 pgg45 0.28 0.16 1.74 0.09 The features \\(\\texttt{gleason}\\), \\(\\texttt{age}\\), and possibly \\(\\texttt{pgg45}\\) and \\(\\texttt{lcp}\\) are non-significant in this model, although note that these variables in particular were highly correlated with each other. This example also illustrates the adverse effect that this multicollinearity can have on a regression model. Even though \\(\\texttt{lcp}\\) was observed to have a fairly strong positive linear relationship with the response variable (\\(r = 0.49\\), third highest of all features), the coefficient estimate is in fact negative, relatively significantly (p = 0.07)! Likewise, even though \\(\\texttt{age}\\) is positively correlated with \\(\\texttt{lpsa}\\) (\\(r = 0.23\\)), its coefficient estimate is negative. Let us now apply \\(L_2\\) regularisation using the glmnet package in R. See section 3.3 for the discussion of the \\(\\alpha\\) parameter. For now, note that \\(\\alpha = 0\\) corresponds to ridge regression. library(glmnet) ridge &lt;- glmnet(x, y, alpha = 0, standardize = T, lambda = exp(seq(-4, 5, length.out = 100))) plot(ridge, xvar = &#39;lambda&#39;, label = T) Figure 3.2: Coefficient profiles for ridge regression on the prostate cancer dataset Here we see how the coefficients vary as \\(\\log(\\lambda)\\) is increased, whilst the labels at the top indicate the number of nonzero coefficients for various values of \\(\\log(\\lambda)\\). Note that none of the coefficients actually equal zero, illustrating that ridge regression does not necessarily perform variable selection per se. In Figure 3.2 we observe that the initially negative coefficient for \\(\\texttt{lcp}\\) \\((\\beta_6)\\) becomes both positive and more significant, relative to the other predictors. Therefore, the notion of “coefficients shrinking towards zero” is a slight misnomer, or perhaps an oversimplification of the effect \\(L_2\\) regularisation has. Eventually, as \\(\\lambda \\to \\infty\\) (or, equivalently, \\(\\tau \\to 0\\) as shown in (3.8)), all coefficients will indeed be forced towards zero. However, the ideal model will usually correspond to a level of \\(\\lambda\\) that allows stronger predictors to be more prominent by diminishing the effect of their correlates. So, how do we determine the appropriate level of \\(\\lambda\\)? By viewing this tuning parameter as a proxy for complexity and applying the same approach as in Chapter 2, we can use CV with the MSE as loss function to identify optimal complexity. #Apply 10-fold CV set.seed(1) ridge_cv &lt;- cv.glmnet(as.matrix(x), y, #this function requires x to be a matrix alpha = 0, nfolds = 10, type.measure = &#39;mse&#39;, standardise = T, lambda = exp(seq(-4, 5, length.out = 100))) #Default lambda range doesn&#39;t cover minimum plot(ridge_cv) abline(h = ridge_cv$cvup[which.min(ridge_cv$cvm)], lty = 2) Figure 3.3: 10-fold CV MSEs as a function of \\(\\log(\\lambda)\\) for ridge regression applied to the prostate cancer dataset Figure 3.3 shows the CV errors (red dots), with the error bars indicating the extent of dispersion of the MSE across folds, the default display being one standard deviation above and below the average MSE. Two values of the tuning parameter are highlighted: the one yielding the minimum CV error (lambda.min), and the one corresponding to the most regularised model such that the error is within one standard error of the minimum (lambda.1se), which has been indicated on this plot with the horizontal dashed line. The choice of \\(\\lambda\\) depends on various factors, including the size of the data set, the length of the resultant error bars, and the profile of the coefficient estimates. In Figure 3.2 we saw that a more “reasonable” representation of the coefficients is achieved when \\(\\log(\\lambda)\\) is closer to zero, rather than at the minimum CV MSE. Showing this explicitly, below we see that the coefficients corresponding to lambda.min (left) still preserves the contradictory coefficient sign for \\(\\texttt{lcp}\\), whereas lambda.1se (right) rectifies this whilst mostly maintaining the overall relative importance across the features, hence we will use the latter. round(cbind(coef(ridge_cv, s = &#39;lambda.min&#39;), coef(ridge_cv, s = &#39;lambda.1se&#39;)), 3) ## 9 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 s1 ## (Intercept) 0.173 -0.181 ## lcavol 0.515 0.284 ## lweight 0.606 0.470 ## age -0.016 -0.002 ## lbph 0.140 0.099 ## svi 0.696 0.492 ## lcp -0.140 0.038 ## gleason 0.007 0.071 ## pgg45 0.008 0.004 Note that although some predictors have almost been removed, these coefficients are still nonzero. Therefore, the ridge regression will include all \\(p\\) predictors in the final model. The CV MSE for the chosen model, which is an estimate of out-of-sample performance, is 0.64. Before using the ridge regression to predict values for the test set, we will first consider the lasso as an approach for variable selection. 3.2.3 The Lasso – \\(L_1\\) Lasso is an acronym that stands for least absolute shrinkage and selection operator. It is another form of regularisation that, similar to ridge regression, attempts to minimise a penalised RSS. However, the constraint is slightly different: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_L = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2, \\\\ \\text{subject to } \\sum_{j=1}^p|\\beta_j| \\leq \\tau. \\tag{3.11} \\end{equation}\\] Or, written in its Lagrangian form: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_L = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}}\\left\\{ \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}. \\tag{3.12} \\end{equation}\\] Again we can see that the equivalent name “\\(L_1\\) regularisation” arises from the fact that the penalty is based on the \\(L_1\\) norm5 \\(||\\boldsymbol{\\beta}||_1 = \\sum_{i=1}^p |\\beta_i|\\). This constraint on the regression parameters makes the solutions nonlinear in the \\(y_i\\), such that there is no closed form expression for \\(\\hat{\\boldsymbol{\\beta}}_L\\) like there is for the ridge estimate, except in the case of orthonormal covariates. Computing the lasso estimate is a quadratic programming problem, although efficient algorithms have been developed to compute the solutions as a function of \\(\\lambda\\) at the same computational cost as for ridge regression. These solutions are beyond the scope of this course. Note that if we let \\(\\tau &gt; \\sum_{j=1}^p|\\hat{\\beta}^{LS}_j|\\), where \\(\\hat{\\beta}^{LS}_j\\) denotes the least squares estimates, then the lasso estimates are exactly equal to the least squares estimates. If, for example, \\(\\tau = \\frac{1}{2} \\sum_{j=1}^p|\\hat{\\beta}^{LS}_j|\\), then the least squares coefficients are shrunk by 50% on average. However, the nature of the shrinkage is not obvious. When comparing ridge regression with the lasso, we see that the nature of the constraints yield different trajectories for \\(\\hat{\\boldsymbol{\\beta}}\\) as \\(\\lambda\\) increases/\\(\\tau\\) decreases: Figure 3.4: Estimation picture for the lasso (left) and ridge regression (right). Shown are contours of the error and constraint functions. The solid blue areas are the constraint regions \\(|\\beta_1| + |\\beta_2| \\leq \\tau\\) and \\(\\beta_1^2 + \\beta_2^2 \\leq \\tau^2\\), respectively, while the red ellipses are the contours of the least squares error function. Source: Hastie et al. (2009), p. 71. As the penalty increases, the lasso constraint sequentially forces the coefficients across the p dimensions onto their respective axes. Let us return to the previous example to illustrate this effect. 3.2.4 Example 2 – Prostate Cancer (continued) Applying \\(L_1\\) regularisation via glmnet follows exactly the same process as for ridge regression, except that we now set \\(\\alpha = 1\\) within the glmnet() function. library(glmnet) lasso &lt;- glmnet(x, y, alpha = 1, standardize = T) plot(lasso, xvar = &#39;lambda&#39;, label = T) Figure 3.5: Coefficient profiles for lasso regression on the prostate cancer dataset Figure 3.5 shows the coefficients shrinking and equaling zero as the regularisation penalty increases, as opposed to gradually decaying as in ridge regression, thereby performing variable selection in the process. Interestingly, here it seems that one of the first variables excluded from the model is \\(\\texttt{lcp}\\), although it is quite difficult to see, even for this small example where \\(p=8\\). In order to determine which variables should be (de)selected, we will again implement CV using the MSE as loss function. #Apply 10-fold CV set.seed(1) lasso_cv &lt;- cv.glmnet(as.matrix(x), y, #this function requires x to be a matrix alpha = 1, nfolds = 10, type.measure = &#39;mse&#39;, standardise = T) plot(lasso_cv) Figure 3.6: 10-fold CV MSEs as a function of \\(\\log(\\lambda)\\) for lasso regression applied to the prostate cancer dataset We can now achieve a notably simpler model where three of the eight coefficients have been shrunk to zero by once again selecting the penalty corresponding to the largest MSE within one standard error of the minimum MSE, as opposed to the minimum MSE where the contradictory estimates will clearly still remain. round(coef(lasso_cv, s = &#39;lambda.1se&#39;), 3) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 0.083 ## lcavol 0.459 ## lweight 0.454 ## age . ## lbph 0.048 ## svi 0.349 ## lcp . ## gleason . ## pgg45 0.001 Once again, the CV MSE for the chosen model, is 0.64. At this point, we have once again defined \\(\\hat{f}\\), which is ultimately still a linear model with adjusted coefficient estimates. Now we can compare how the ridge and lasso models fare on the test set, which we can compare to the OLS linear model too. test_y &lt;- test_pros[, 9] test_x &lt;- as.matrix(test_pros[, -9]) #need to extract just the x&#39;s for glmnet predict function test_x_stand &lt;- scale(test_x) #standardise for lm test_pros_stand &lt;- data.frame(test_x_stand, lpsa = test_y) #Yhats ridge_pred &lt;- predict(ridge_cv, test_x, s = &#39;lambda.1se&#39;) lasso_pred &lt;- predict(lasso_cv, test_x, s = &#39;lambda.1se&#39;) ols_pred &lt;- pred &lt;- predict(lm_full, test_pros_stand) #Test MSEs ridge_mse &lt;- round(mean((test_y - ridge_pred)^2), 3) lasso_mse &lt;- round(mean((test_y - lasso_pred)^2), 3) ols_mse &lt;- round(mean((test_y - ols_pred)^2), 3) # Unregularised linear model ols_mse ## [1] 0.55 # Ridge regression ridge_mse ## [1] 0.51 # The lasso lasso_mse ## [1] 0.45 The final results show that the unregularised linear model performed worst, and the lasso performed best in this example. It should be noted that the accuracy of these predictions in context of the application should always be evaluated in consultation with the subject experts, i.e. the oncologist in this instance. 3.3 Elastic-net When applying both ridge regression and the lasso, we needed to specify an \\(\\alpha\\) parameter. This is because both of these can be seen as special cases (the extremes) of the elastic-net penalty, a mixture of the two penalties discussed above, first proposed by Zou &amp; Hastie (2005): \\[\\begin{equation} \\text{penalty} = \\lambda \\left[ (1-\\alpha)\\left(\\sum_{j=1}^p \\beta_j^2\\right) + \\alpha\\left(\\sum_{j=1}^p |\\beta_j|\\right) \\right] \\tag{3.13} \\end{equation}\\] Note that the \\(\\alpha\\) terms have actually been swapped around here in order to correspond with the application in glmnet. The elastic-net selects variables like the lasso, and shrinks together the coefficients of correlated predictors like ridge. Of course, now there are two parameters to tune simultaneously, and the choice of \\(\\alpha\\) influences the range of \\(\\lambda\\) values we should consider searching over – compare the x-axis ranges of Figures 3.3 and 3.6. At time of writing, to the author’s knowledge, there is no R package specifically for elastic-net on CRAN that allows one to automatically search over both tuning parameters, or hyperparameters and find the optimal combination. Therefore, there are three options: Manually vary and loop across various \\(\\alpha\\) values, each time extracting the optimal \\(\\lambda\\) and identifying the lowest overall CV MSE. Use a non-CRAN package that has already written up exactly this procedure, for example glmnetUtils. Use a wrapper function designed for hyperparameter gridsearches. caret is an excellent R package for this purpose, one we will use going forward in this course. Implementing this and comparing the results to the above is left as a homework exercise. 3.4 Homework exercises Show that the penalised RSS for the ridge regression yields \\(\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}&#39;\\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}\\boldsymbol{y}\\). Apply elastic-net regression to the prostate cancer dataset using one of the suggested methods in section 3.3. In this context, statisticians often prefer the term co-variates.↩︎ Also called the Euclidean norm, or the length of a vector in Euclidean space.↩︎ In other words, the big weights are shrunk more than the small weights, and when rescaling the features the relative sizes of the weights change.↩︎ Also referred to as the Manhattan norm/distance.↩︎ "],["references.html", "References", " References Abu-Mostafa, Y. S., Magdon-Ismail, M., &amp; Lin, H.-T. (2012). Learning from data. AMLBook. Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: Data mining, inference, and prediction (Vol. 2). Springer. James, G., Witten, D., Hastie, T., Tibshirani, R., et al. (2013). An introduction to statistical learning (Vol. 112). Springer. Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301–320. http://www.jstor.org/stable/3647580 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
