[["index.html", "Chapter 1 Introduction", " STA4026S – Honours Analytics Section B: Theory and Application of Supervised Learning Stefan S. Britz August 2023 Chapter 1 Introduction Welcome to an introduction to supervised learning! In this set of course notes we will: cover some of the fundamental theoretical principles underpinning supervised statistical and machine learning; explore various models, algorithms, and heuristics to analyse different types of data, both for regression (continuous target variable) and classification (categorical target variable) problems; and apply these methods in R. The aim is to find a balance between breadth of topics, depth of theory, and practical application. Since we will be covering several topics in a relatively short time, the application component will focus largely on the current best practices for implementation in R. Therefore, we will mostly be using existing R packages and will not spend time coding these algorithms from scratch, with one exception in Chapter 6. The fields of statistical learning/AI/machine learning/data science/analytics/data mining/deep learning/[insert new buzzword here] are constantly evolving at a rapid pace. Although the core theory and methodology will (should) always be relevant, adaptations to the methods are regularly being developed, along with more efficient and convenient packages for implementation. Therefore, although these notes attempt to introduce you to up-to-date modern frameworks, note that these things change over time. Also note that this is by no means an exhaustive exploration of either theory, methods, or application, but it will imbue you with a skill set with which to tackle various problems and provide a solid foundation for further learning. These notes draw from various sources, with the theoretical aspects largely relying on An Introduction to Statistical Learning with Applications in R (James et al., 2013) and Elements of Statistical Learning (Hastie et al., 2009), both of which are freely available here and here, respectively. It is recommended that you keep the former on hand, as you will be referred to sections therein for reading. Other sources will be referenced as and when they are used. Happy learning! "],["supervised-learning.html", "Chapter 2 Supervised Learning 2.1 Bias-Variance trade-off 2.2 Model validation 2.3 Side note: Statistical learning vs machine learning", " Chapter 2 Supervised Learning This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, most of which you should be familiar with already. Using a set of observations to uncover some underlying process in the real world is the basic premise of “learning from data” (Abu-Mostafa et al., 2012, p. 11). By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course. The basic distinction is that supervised learning refers to cases where there is some target variable, usually indicated as \\(Y\\), whereas if we are interested in the structures and patterns in explanatory variables1 only, we refer to unsupervised learning. Given a quantitative response \\(Y\\) and a set of \\(p\\) predictors \\(X_1, X_2, \\ldots, X_p\\), we are interested in the assumed, unobserved function that maps the inputs to the outputs: \\[\\begin{equation} Y = \\underbrace{f(X)}_{systematic} + \\underbrace{\\epsilon}_{random}, %\\tag{2.1} \\end{equation}\\] where \\(f(.)\\) represents the fixed, but unknown function and \\(\\epsilon\\) is a random error term, independent of \\(X\\), with \\(E(X) = 0\\). By estimating \\(f\\) such that \\[\\begin{equation} \\hat{Y} = \\hat{f}(X), %\\tag{2.2} \\end{equation}\\] we allow for both prediction of \\(Y\\) – which is the primary goal in forecasting – and inference, i.e. describing how \\(Y\\) is affected by changes in \\(X\\). Estimating \\(\\hat{f}\\) can be done in two ways, namely via a parametric or a non-parametric approach. Parametric approach Here an assumption is made about the the functional form of \\(f\\), for example \\[f(\\boldsymbol{X}) = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j.\\] The best estimate of \\(f\\) is now defined as the set of parameters \\(\\hat{\\boldsymbol{\\beta}}\\) that minimise some specified loss function. Given a set of observations \\(\\mathcal{D}=\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^n\\), we could use ordinary least squares to minimise the mean squared error (MSE): \\[MSE = \\frac{1}{n}\\sum_{i=1}^n\\left[y_i - \\hat{f}(x_i)\\right]^2 \\] Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters. Non-parametric approach Another option is to make no explicit assumptions regarding the functional form of \\(f\\). This allows one to fit a wide range of possible forms for \\(f\\) – in these notes we consider K-nearest neighbours and tree-based methods – but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation. The objective remains to find \\(f\\) that fits the available data as closely as possible, although one must be careful to avoiding overfitting, ensuring that the model generalises well to unseen data. Generalisation The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model. By now you will be familiar with the 2.1 Bias-Variance trade-off Consider the case where \\(\\hat{f}\\) is fixed and out-of-sample observations of the variables are introduced, which we will denote as {_0, y_0}. We can deconstruct the theoretical MSE as follows: \\[\\begin{align} E\\left[y_0 - \\hat{f}(\\boldsymbol{x}_0) \\right]^2 &amp;= E[f(\\boldsymbol{x}_0) + \\epsilon - \\hat{f}(\\boldsymbol{x}_0)]^2 \\\\ &amp;= E[(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0))^2 + 2\\epsilon(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)) + \\epsilon^2] \\\\ &amp;= E[(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0))^2] + 2E[\\epsilon]E[(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0))] + E[\\epsilon^2] \\\\ &amp;= \\underbrace{E[(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0))^2]}_{reducible} + \\underbrace{Var(\\epsilon)}_{irreducible} \\\\ \\end{align}\\] 2022P1 2.2 Model validation Hoef nie CV van scratch af nie, Et het die beginsel gedek. This has been a very brief summary of some of the basic statistical learning principles we will encounter going forward. For a more in-depth discussion, 2.3 Side note: Statistical learning vs machine learning Statistical Learning: Foundation: Statistical learning is rooted in the field of statistics and builds upon principles of probability, inference, and hypothesis testing. Objective: The primary objective of statistical learning is to understand and model relationships between variables in order to make predictions or draw insights from data. Emphasis: Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. Assumptions: It often involves making explicit assumptions about the underlying statistical distributions and relationships between variables. Interpretability: Statistical models tend to be more interpretable, as they often involve understanding the impact and significance of individual variables on the outcome. Foundation: Machine learning draws from various fields, including computer science, artificial intelligence, and statistical learning, but with a stronger emphasis on algorithmic and computational aspects. Objective: The primary objective of machine learning is to develop algorithms that can automatically learn patterns and relationships from data, with an emphasis on predictive accuracy and generalization to new, unseen data. Emphasis: While understanding the underlying data-generating process is important, machine learning is often more focused on achieving optimal predictive performance. Assumptions: Machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions. Interpretability: Some machine learning models, like deep neural networks, can be highly complex and challenging to interpret due to their architecture and the multitude of learned features. It’s important to note that the distinction between statistical learning and machine learning can sometimes be blurred, and the two fields have evolved to influence each other over time. Many modern machine learning techniques, such as ensemble methods and regularization, have strong roots in statistical learning theory. Additionally, both fields share common goals of extracting insights from data and making predictions, but they may approach these goals with different philosophical and methodological perspectives. In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance. Often referred to as features in the learning context, or predictors in supervised learning specifically.↩︎ "],["model-selection-regularisation.html", "Chapter 3 Model Selection &amp; Regularisation 3.1 L1 and L2 3.2 ElasticNet 3.3 PCR &amp; PLS", " Chapter 3 Model Selection &amp; Regularisation Noem Best subset &amp; stepwise, maar geen detail 3.1 L1 and L2 Bring teorie in vir \\(\\hat{\\beta_{RIDGE}}\\) en Lasso. Kyk na Othonormal covariates op Lasso wiki 3.2 ElasticNet 3.3 PCR &amp; PLS Sal sien oor die PLS, hang af van die tyd en scope.Versigtig vir ISLR, nie noodwendig reg nie. A3Q2 "],["classification-models.html", "Chapter 4 Classification Models 4.1 Logistic regression 4.2 Model evaluation", " Chapter 4 Classification Models 4.1 Logistic regression 4.2 Model evaluation 4.2.1 ROC Curves 4.2.2 PR curves "],["beyond-linearity.html", "Chapter 5 Beyond Linearity 5.1 GAMs 5.2 WLS - LOESS/LOWESS??? 5.3 KNN", " Chapter 5 Beyond Linearity Begin met polynomial Polynomial regression? Noem dit maar net. Dalk vroeër, om die CV beginsel te illustreer? 5.1 GAMs A3Q3? 5.2 WLS - LOESS/LOWESS??? Well, I like it… 5.3 KNN "],["tree-based-methods.html", "Chapter 6 Tree-based Methods 6.1 CART 6.2 Bagging &amp; RF 6.3 Boosting", " Chapter 6 Tree-based Methods 6.1 CART Murphy het ook ’n nice verduideliking hieroor in 16.2. Iris is dalk ook ’n goeie illustrative voorbeeld. 6.2 Bagging &amp; RF 6.3 Boosting Gebruik Murphy 16.4 as die basis hiervoor Vir application gebruik caret en tidymodels (met tune, fine, workflowsets). Daar is ook nuwe goed soos hierdie https://arxiv.org/abs/2303.12177 "],["references.html", "References", " References Abu-Mostafa, Y. S., Magdon-Ismail, M., &amp; Lin, H.-T. (2012). Learning from data. AMLBook. Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: Data mining, inference, and prediction (Vol. 2). Springer. James, G., Witten, D., Hastie, T., Tibshirani, R., et al. (2013). An introduction to statistical learning (Vol. 112). Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
