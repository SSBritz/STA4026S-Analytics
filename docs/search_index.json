[["index.html", "Chapter 1 Introduction", " STA4026S – Honours Analytics Section B: Theory and Application of Supervised Learning Stefan S. Britz August 2023 Chapter 1 Introduction Welcome to an introduction to supervised learning! In this set of course notes we will: cover some of the fundamental theoretical principles underpinning supervised statistical and machine learning; explore various models, algorithms, and heuristics to analyse different types of data, both for regression (continuous target variable) and classification (categorical target variable) problems; and apply these methods in R. The aim is to find a balance between breadth of topics, depth of theory, and practical application. Since we will be covering several topics in a relatively short time, the application component will focus largely on the current best practices for implementation in R. Therefore, we will mostly be using existing R packages and will not spend time coding these algorithms from scratch, with one exception in Chapter 6. The fields of statistical learning/AI/machine learning/data science/analytics/data mining/deep learning/[insert new buzzword here] are constantly evolving at a rapid pace. Although the core theory and methodology will (should) always be relevant, adaptations to the methods are regularly being developed, along with more efficient and convenient packages for implementation. Therefore, although these notes attempt to introduce you to up-to-date modern frameworks, note that these things change over time. Also note that this is by no means an exhaustive exploration of either theory, methods, or application, but it will imbue you with a skill set with which to tackle various problems and provide a solid foundation for further learning. These notes draw from various sources, with the theoretical aspects largely relying on An Introduction to Statistical Learning with Applications in R (James et al., 2013) and Elements of Statistical Learning (Hastie et al., 2009), both of which are freely available here and here, respectively. It is recommended that you keep the former on hand, as you will be referred to sections therein for reading. Other sources will be referenced as and when they are used. Happy learning! "],["supervised-learning.html", "Chapter 2 Supervised Learning 2.1 Bias-Variance trade-off 2.2 Model validation 2.3 Side note: Statistical learning vs machine learning 2.4 Homework exercises", " Chapter 2 Supervised Learning This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, some of which you should be familiar with already. Whereas there are many good sources that provide a more comprehensive discussion, Chapter 2 of James et al. (2013) is sufficient for the level and scope of this course. Using a set of observations to uncover some underlying process in the real world is the basic premise of “learning from data” (Abu-Mostafa et al., 2012, p. 11). By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course. The basic distinction is that supervised learning refers to cases where there is some target variable, usually indicated as \\(Y\\), whereas if we are interested in the structures and patterns in explanatory variables1 only, we refer to unsupervised learning. Given a quantitative response \\(Y\\) and a set of \\(p\\) predictors \\(X_1, X_2, \\ldots, X_p\\), we are interested in the assumed, unobserved function that maps the inputs to the outputs: \\[Y = \\underbrace{f(X)}_{systematic} + \\underbrace{\\epsilon}_{random},\\] where \\(f(.)\\) represents the fixed, but unknown function and \\(\\epsilon\\) is a random error term, independent of \\(X\\), with \\(E(X) = 0\\). By estimating \\(f\\) such that \\[\\hat{Y} = \\hat{f}(X),\\] we allow for both prediction of \\(Y\\) – which is the primary goal in forecasting – and inference, i.e. describing how \\(Y\\) is affected by changes in \\(X\\). Hypothesising \\(\\hat{f}\\) can be done in two ways, namely via a parametric or a non-parametric approach. Parametric approach Here an assumption is made about the the functional form of \\(f\\), for example \\[f(\\boldsymbol{X}) = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j.\\] The best estimate of \\(f\\) is now defined as the set of parameters \\(\\hat{\\boldsymbol{\\beta}}\\) that minimise some specified loss function. Given a set of observations \\(\\mathcal{D}=\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^n\\) – henceforth referred to as the training set – we could use ordinary least squares to minimise the mean squared error (MSE): \\[MSE = \\frac{1}{n}\\sum_{i=1}^n\\left[y_i - \\hat{f}(x_i)\\right]^2 \\] Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters. Non-parametric approach Another option is to make no explicit assumptions regarding the functional form of \\(f\\). This allows one to fit a wide range of possible forms for \\(f\\) – in these notes we consider K-nearest neighbours and tree-based methods – but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation. The objective remains to find \\(f\\) that fits the available data as closely as possible, whilst avoiding overfitting to ensure that the model generalises well to unseen data. Generalisation The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model, referred to as out-of-sample observations. Consider the case where the estimated function \\(\\hat{f}\\) is fixed and out-of-sample observations of the variables are introduced, which we will denote as \\(\\{\\boldsymbol{x}_0, y_0\\}\\). The expected MSE for these test set observations (see Section 2.2) can be deconstructed as follows: \\[\\begin{align} E\\left[y_0 - \\hat{f}(\\boldsymbol{x}_0) \\right]^2 &amp;= E\\left[f(\\boldsymbol{x}_0) + \\epsilon - \\hat{f}(\\boldsymbol{x}_0)\\right]^2 \\\\ &amp;= E\\left[\\left(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right)^2 + 2\\epsilon \\left(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0) \\right) + \\epsilon^2\\right] \\\\ &amp;= E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right]^2 + 2E[\\epsilon]E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right] + E\\left[\\epsilon^2\\right] \\\\ &amp;= \\underbrace{E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right]^2}_{reducible} + \\underbrace{Var(\\epsilon)}_{irreducible} \\tag{2.1} \\end{align}\\] The primary goal of machine learning is to find an \\(\\hat{f}\\) that best approximates the underlying, unknown relationship between the input and output variables by minimising the reducible error. Note that because of the irreducible component (the “noise” in the data), there will always be some lower bound for the theoretical MSE, and that this bound is almost always unknown in practice (James et al., 2013, p. 19). By now you will be familiar with the concept of bias-variance trade-off, according to which we attempt to find a sufficiently (but not overly) complex model. The reducible error component can be decomposed further to help illustrate this trade-off. 2.1 Bias-Variance trade-off Consider again a fixed \\(\\hat{f}\\) and out-of-sample observations \\(\\{\\boldsymbol{x}_0, y_0\\}\\). For ease of notation, let \\(f = f(\\boldsymbol{x}_0)\\) and \\(\\hat{f} = \\hat{f}(\\boldsymbol{x}_0)\\). Also note that \\(f\\) is deterministic such that \\(E\\left[f\\right] = f\\). Starting with the reducible error in Equation (2.1), we have \\[\\begin{align} E\\left[f - \\hat{f} \\right]^2 &amp;= E\\left[\\hat{f} - f \\right]^2 \\\\ &amp;= E\\left[\\hat{f} - E(\\hat{f}) + E(\\hat{f}) - f \\right]^2 \\\\ &amp;= E\\left\\{ \\left[\\hat{f} - E(\\hat{f})\\right]^2 + 2\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right] + \\left[E(\\hat{f}) - f\\right]^2 \\right\\} \\\\ &amp;= E\\left[\\hat{f} - E(\\hat{f})\\right]^2 + 2E\\left\\{\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right]\\right\\} + E\\left[E(\\hat{f}) - f\\right]^2 \\\\ &amp;= Var\\left[\\hat{f}\\right] + 0 + \\left[E(\\hat{f}) - f\\right]^2 \\\\ &amp;= Var\\left[\\hat{f}\\right] + Bias^2\\left[\\hat{f}\\right] \\tag{2.2} \\end{align}\\] Showing that the crossproduct term equals zero: \\[\\begin{align} E\\left\\{\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right]\\right\\} &amp;= E\\left[\\hat{f}E(\\hat{f}) - E(\\hat{f})E(\\hat{f}) - \\hat{f}f + E(\\hat{f})f\\right] \\\\ &amp;= E(\\hat{f})E(\\hat{f}) - E(\\hat{f})E(\\hat{f}) - E(\\hat{f})f + E(\\hat{f})f \\\\ &amp;= 0 \\end{align}\\] Therefore, in order to minimise the expected test MSE we need to find a model that has the lowest combined variance and (squared) bias. The variance represents the extent to which \\(\\hat{f}\\) changes between different training samples taken from the same population. The bias of \\(\\hat{f}\\) is simply the error that is introduced by approximating the real-world relationship with a simpler representation. Note, however, that since \\(f\\) is generally unknown, the bias component cannot be directly observed or measured outside of simulations. However, these simulations may help us illustrate how the bias and variance change as model complexity increases. Although the concepts of model complexity and flexibility are not necessarily perfectly defined – depending on the class of model being hypothesised – the following example should provide an intuitive understanding. 2.1.1 Example 1 – Simulation To allow for easy visualisation, let us consider a simple model with only one feature: \\[Y = X + 2\\cos(5X) + \\epsilon,\\] where \\(\\epsilon \\sim N(0, 2)\\). Below we simulate \\(n = 100\\) observations from \\(X \\sim U(-2,2)\\), to which we fit cubic smoothing splines of increasing complexity. [Splines are beyond the scope of this course, but provide an easy-to-see illustration of “flexibility”] set.seed(4026) #Simulated data x &lt;- runif(100, -2, 2) y &lt;- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2)) #The true function xx &lt;- seq(-2, 2, length.out = 1000) f &lt;- xx + 2*cos(5*xx) #Fit cubic splines with increasing degrees of freedom for(dof in 2:50){ fhat &lt;- smooth.spline(x, y, df = dof) plot(x, y, pch = 16) lines(xx, f, &#39;l&#39;, lwd = 2) lines(fhat, col = &#39;blue&#39;, lwd = 2) title(main = paste(&#39;Degrees of freedom:&#39;, dof)) legend(&#39;bottomright&#39;, c(&#39;f(x) - True&#39;, expression(hat(f)(x) ~ &#39;- Cubic spline&#39;)), col = c(&#39;black&#39;, &#39;blue&#39;), lty = 1, lwd = 2) } This serves to illustrate how the model’s degrees of freedom is directly proportional to the model’s complexity. However, to extricate the bias and variance components, we need to observe these models’ fit on out-of-sample data across many random realisations of training samples. In the following simulation we again observe \\(n=100\\) training observations at a time, to which the same models of varying complexity as above are fitted. Each model’s fit is then assessed on a set of 100 testing observations, where the \\(\\boldsymbol{x}_0\\) (and, therefore, true \\(f(\\boldsymbol{x}_0)\\)) are fixed, but random noise is added. This process is repeated 1000 times, such that we can keep track of how each test observation’s predictions vary across the iterations, as well as the errors. set.seed(1) n &lt;- 100 #Sample size num_sims &lt;- 1000 #Number of iterations (could be parallelised) dofs &lt;- 2:25 #Model complexities var_eps &lt;- 2 #Var(epsilon): The irreducible error pred_mat &lt;- matrix(nrow = num_sims, ncol = n) #To store each set of predictions mses &lt;- vector(length = num_sims) #Also want to track the testing MSEs red_err &lt;- vector(length = num_sims) #As well as the reducible error #Herein we will capture the deconstructed components for each model results &lt;- data.frame(Var = NA, Bias2 = NA, Red_err = NA, MSE = NA) #Testing data x_test &lt;- runif(n, -2, 2) f_test &lt;- x_test + 2*cos(5*x_test) #This is the part we don&#39;t know outside sims!! d &lt;- 0 #To keep track of dof iterations, even when changing the range for(dof in dofs) { #Repeat over all model complexities d &lt;- d+1 for(iter in 1:num_sims){ #Training data x_train &lt;- runif(n, -2, 2) y_train &lt;- x_train + 2*cos(5*x_train) + rnorm(n, sd = sqrt(var_eps)) #Add the noise y_test &lt;- f_test + rnorm(n, sd = sqrt(var_eps)) #Fit cubic spline spline_mod &lt;- smooth.spline(x_train, y_train, df = dof) #Predict on OOS data yhat &lt;- predict(spline_mod, x_test)$y #And store pred_mat[iter, ] &lt;- yhat red_err[iter] &lt;- mean((f_test - yhat)^2) mses[iter] &lt;- mean((y_test - yhat)^2) } #Average each component over all iterations var_fhat &lt;- mean(apply(pred_mat, 2, var)) #E[\\hat{f} - E(\\hat{f})]^2 bias2_fhat &lt;- mean((colMeans(pred_mat) - f_test)^2) #E[E(\\hat{f}) - f]^2 reducible &lt;- mean(red_err) #E[f - \\hat{f}]^2 MSE &lt;- mean(mses) #E[y_0 - \\hat{f}]^2 results[d, ] &lt;- c(var_fhat, bias2_fhat, reducible, MSE) } #Plot the results plot(dofs, results$MSE, &#39;l&#39;, col = &#39;darkred&#39;, lwd = 2, xlab = &#39;Model complexity&#39;, ylab = &#39;&#39;, ylim = c(0, max(results))) lines(dofs, results$Bias2, &#39;l&#39;, col = &#39;lightblue&#39;, lwd = 2) lines(dofs, results$Var, &#39;l&#39;, col = &#39;orange&#39;, lwd = 2) lines(dofs, results$Red_err, &#39;l&#39;, lty = 2, lwd = 2) legend(&#39;topright&#39;, c(&#39;MSE&#39;, expression(Bias^2 ~ (hat(f))), expression(Var(hat(f))), &#39;Reducible Error&#39;), col = c(&#39;darkred&#39;, &#39;lightblue&#39;, &#39;orange&#39;, &#39;black&#39;), lty = c(rep(1, 3), 2), lwd = 2) abline(v = dofs[which.min(results$MSE)], lty = 3) #Complexity minimising MSE abline(h = var_eps, lty = 3) #MSE lower bound Figure 2.1: I must still write this figure caption!! As a quick sanity check before interpreting this result, let us add up the components – which were calculated separately – and see whether we indeed observe that \\(E\\left[f - \\hat{f} \\right]^2 = Var\\left[\\hat{f}\\right] + Bias^2\\left[\\hat{f}\\right]\\) and \\(\\text{Test MSE} = E\\left[y_0 - \\hat{f}\\right]^2 = E\\left[f - \\hat{f} \\right]^2 + Var(\\epsilon)\\). Note that we will need to have a small tolerance for discrepancy, since we have approximated the expected values by averaging over only 1000 realisations. This approximation will become more accurate as the number of iterations is increased. #Is reducible error = var(fhat) + bias^2(fhat)? ifelse(isTRUE(all.equal(results$Red_err, results$Var + results$Bias2, tolerance = 0.001)), &#39;Happy days! :D&#39;, &#39;Haibo...&#39;) ## [1] &quot;Happy days! :D&quot; #Is Test MSE = var(fhat) + bias^2(fhat) + var(eps)? ifelse(isTRUE(all.equal(results$MSE, results$Var + results$Bias2 + var_eps, tolerance = 0.01)), &#39;Happy days! :D&#39;, &#39;Haibo...&#39;) ## [1] &quot;Happy days! :D&quot; Figure 2.1 illustrates the general error pattern when learning from data: As model complexity/flexibility increases, the variance across multiple training samples increases, whilst the (squared) bias decreases as the estimated function gets closer to the true pattern on average. Note that \\(E(\\epsilon^2) = Var(\\epsilon)\\) remains constant. This decrease in bias\\(^2\\) initially offsets the increase in variance such that the test MSE initially decreases. However, from some complexity/flexibility of \\(\\hat{f}\\), the decrease in bias\\(^2\\left(\\hat{f}\\right)\\) is offset by the increase in \\(Var\\left(\\hat{f}\\right)\\), at which point the model starts to overfit and the test MSE starts increasing. This is the bias-variance trade-off. In this particular example, we see that of all the cubic splines, one with 13 degrees of freedom most closely captures the underlying pattern in the data, as measured by the test MSE. The fundamental challenge in statistical learning is to postulate a model of the data that yields both a low bias and variance, whilst policing the model complexity such that the sum of these error components are minimised. In the above example, we knew what the underlying function was as well as the residual variance. However, when modelling data generated in some real-world environment, we do not observe \\(f\\) and therefore cannot explicitly compute the test MSE. In order to estimate the test MSE, we make use of model validation procedures. 2.2 Model validation Imagine there are two students who have been subjected to the same set of lectures, notes, and homework exercises, which you can view as their training data used to learn the true subject knowledge. When studying for the test – which is designed to test this knowledge, i.e. the test set in our analogy – they take two different approaches: Student A, a model student, tries to master the subject matter by focusing on the course material, testing themself with new homework exercises after studying some completed ones first. Student B, however, managed to obtain a copy of the test in advance through some nefarious means, and plans to prove their knowledge of the subject matter by preparing only for this specific set of questions. Even though student B’s test score will in all likelihood be better, does this mean that they have obtained and retained more knowledge? Certainly not! Suppose the lecturer catches wind of this cheating and swaps the initial test with a new set of randomised questions. Which of the two approaches would you expect to yield better results on such a test, on average? When comparing different statistical models, we would like to select the one that we think will work best on unseen test data. But if we use the test data to make this decision, this will also be cheating, and we will be no better off for it. Like student A though, we can leave out some exercises in the training data and use these to validate our learning, i.e. gauge how well we would do in the test. 2.2.1 Validation set One way to create a validation set (or hold-out set) is to just leave aside, in a randomised way, a portion of the training data, say 30%. We then train models on the other 70% of the data only, test them on the validation set, and select the model that yields the lowest validation MSE, which serves as an estimate of test set performance. Although there are some situations in which this approach is merited, it has two potential flaws: Due to the single random split, the validation estimate of the test error can be highly variable. Since we are reducing our training data, the model sees less information, generally leading to worse performance. Therefore, the validation error may overestimate the test error. We will not go into any more detail than this on the validation set approach, but rather focus on cross-validation (CV) strategies, which addresses these two issues. 2.2.2 \\(k\\)-fold CV With this approach, the training set is randomly divided in to \\(k\\) groups, or folds, of (approximately) the same size. Each fold gets a turn to act as the validation set, with the model trained on the remaining \\(k-1\\) folds. Therefore, the training process is repeated \\(k\\) times, each yielding an estimate of the test error, denoted as \\(MSE_1,\\ MSE_2,\\ldots,\\ MSE_k\\). These values are then averaged over all \\(k\\) folds to yield the \\(k\\)-fold CV estimate: \\[CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^k MSE_i\\] The next obvious question is: What value of \\(k\\) should we choose? Start by considering the lowest value, \\(k=2\\). This would be the same as the validation set approach with a 50% split, except that each half of the data will get a chance to act as training and validation set. Therefore, we still expect the validation error to overestimate the test error, or in other words, there will be some bias. As we increase \\(k\\), the estimated error will become more unbiased, since each fold will allow the model to capture more of the underlying pattern. However, just as with model complexity we also need to consider the variance aspect. Consider now the other extreme, when \\(k = n\\). Here we have what is referred to as Leave-one-out cross-validation (LOOCV), since we have \\(n\\) folds, each leaving out just one observation for validation. Each of these \\(n\\) training sets will be almost identical, such that there will be very high correlation between them. Now, remember that when we add correlated random variables (note that averaging involves summation), then the correlation affects the resulting variance: \\[Var(X+Y) = \\sigma^2_X + \\sigma^2_Y + 2\\frac{\\rho_{XY}}{\\sigma_X\\sigma_Y}\\] Therefore, larger \\(k\\) implies larger variation of the estimated error. This means that the same bias-variance trade-off applies to \\(k\\)-fold CV! In practice, it has been shown that \\(k = 5\\) or \\(k = 10\\) yields a good balance such that the test error estimate does not suffer from excessively high bias nor variance. Also note that as \\(k\\) increases, the computational cost increase proportionally, since \\(k\\) separate models must be fitted to \\(k\\) different data splits. This could cause unnecessarily long training times for large datasets/complicated models, such that a smaller \\(k\\) might be preferable. To illustrate the implementation, let us return to the earlier example, where we will pretend that we do not know the underlying relationship we are trying to estimate. 2.2.3 Example 1 – Simulation (continued) Before using the CV error to determine the ideal model complexity, let us first illustrate the concept of cross-validation for a single model, say a cubic spline with 8 degrees of freedom, with \\(k = 10\\). set.seed(4026) #Simulated data x &lt;- runif(100, -2, 2) y &lt;- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2)) cv_k &lt;- c() train_err &lt;- c() #10-fold CV ind &lt;- sample(1:100) #Don&#39;t actually need to randomise here, but should in general for(k in 1:10){ valid_ind &lt;- ind[seq((1+10*(k-1)), 10*k)] fit &lt;- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8) test &lt;- predict(fit, x[valid_ind])$y train_fit &lt;- predict(fit, x[-valid_ind])$y cv_k &lt;- c(cv_k, mean((test - y[valid_ind])^2)) train_err &lt;- c(train_err, mean((train_fit - y[-valid_ind])^2)) #Should write the above into a function... #But the plotting needs to be inside the loop for the rendering par(mfrow = c(1, 2)) plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = &#39;x&#39;, ylab = &#39;y&#39;, xlim = c(min(x), max(x)), ylim = c(min(y), max(y))) points(x[valid_ind], y[valid_ind], pch = 16, col = &#39;gray&#39;) segments(x[valid_ind], y[valid_ind], x[valid_ind], test, col = &#39;red&#39;, lty = 3, lwd = 2) lines(fit, col = &#39;blue&#39;, lwd = 2) title(main = paste(&#39;Fold:&#39;, k)) legend(&#39;bottomright&#39;, c(&#39;Training&#39;, &#39;Validation&#39;, &#39;Errors&#39;), col = c(&#39;black&#39;, &#39;gray&#39;, &#39;red&#39;), pch = c(16, 16, NA), lty = c(NA, NA, 3), lwd = c(NA, NA, 2)) plot(1:k, cv_k, &#39;b&#39;, col = &#39;red&#39;, lwd = 2, xlab = &#39;Fold&#39;, ylab = &#39;MSE&#39;, xlim = c(1, 10), ylim = c(1, 5.5)) lines(1:k, train_err, &#39;b&#39;, lwd = 2) legend(&#39;topright&#39;, c(&#39;Training&#39;, &#39;Validation&#39;), col = c(&#39;black&#39;, &#39;red&#39;), lwd = 2) } Here we see that, as expected, the validation error is noticeably more variable than the training error across the folds. We can calculate the CV MSE as 2.77, although on its own this value is not particularly insightful. When comparing it to that of other models, though, we can determine which model complexity is estimated to yield the lowest test error. #Keep track of MSE per fold, per model fold_mses &lt;- matrix(nrow = 10, ncol = length(dofs)) for(k in 1:10){ cv_ind &lt;- ind[seq((1+10*(k-1)), 10*k)] d &lt;- 0 for(dof in dofs){ #Using the same dofs as earlier d &lt;- d + 1 fit &lt;- smooth.spline(x[-cv_ind], y[-cv_ind], df = dof) test &lt;- predict(fit, x[cv_ind])$y fold_mses[k, d] &lt;- mean((test - y[cv_ind])^2) } } #Average over all folds cv_mses &lt;- colMeans(fold_mses) # Compare the true MSE from earlier plot(dofs, results$MSE, &#39;l&#39;, col = &#39;darkred&#39;, lwd = 2, xlab = &#39;Model complexity&#39;, ylab = &#39;&#39;, ylim = c(0, max(cv_mses))) lines(dofs, cv_mses, &#39;l&#39;, col = &#39;grey&#39;, lwd = 2) legend(&#39;bottomright&#39;, c(&#39;CV MSE&#39;, &#39;True test MSE&#39;), col = c(&#39;gray&#39;, &#39;darkred&#39;), lty = 1, lwd = 2) abline(v = dofs[which.min(results$MSE)], lty = 3) points(dofs[which.min(cv_mses)], min(cv_mses), pch = 13, cex = 2.5, lwd = 2) Because we simulated these data, we know that the cubic spline yielding the lowest expected test MSE is one with 13 degrees of freedom. Applying 10-fold CV to our 100 training data points resulted in an estimated optimal model with 12 degrees of freedom. It is interesting to note that the CV error consistently overestimated the true error. This is likely due to the relatively small dataset; remember that we only tested on 10 observations per fold! The shape of the true MSE curve was captured relatively well by the CV process in this example. This section provided a succinct illustration of model validation. For a detailed discussion, see Section 5.1 of James et al. (2013). In the following chapters we will move beyond simulated data and apply these methods to various datasets as we encounter different classes of models and other techniques. Although there is much value in coding the CV procedure from scratch, it is built into various R packages, which we will leverage going forward. 2.3 Side note: Statistical learning vs machine learning It may seem that we use the terms “statistical learning” and “machine learning” interchangeably, so is there a difference? The distinction between these two concepts can sometimes be blurred with the paradigms largely overlapping, and some might argue that the difference is mostly semantic. In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance. However, the principles discussed in this chapter forms the core of both approaches – both are concerned with extracting insights from data and making predictions, although they may approach these goals with slightly different philosophical and methodological perspectives. Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. While understanding the underlying data-generating process is still important in machine learning, the focus is often more on achieving optimal predictive performance. Statistical learning approaches are also characterised by explicit assumptions about the underlying statistical distributions and relationships between variables, whereas machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions. For the purposes of our study throughout this course, these distinctions are not of consequence and we will adopt both perspectives throughout. 2.4 Homework exercises For Example 1, edit the provided code such that LOOCV is applied. Does this method suggest a different model complexity? Now do the same for 5-fold CV. What changes in the CV curve do you observe as \\(k\\) changes? Often referred to as features in the learning context, or predictors in supervised learning specifically.↩︎ "],["model-selection-regularisation.html", "Chapter 3 Model Selection &amp; Regularisation 3.1 L1 and L2 3.2 ElasticNet 3.3 PCR &amp; PLS", " Chapter 3 Model Selection &amp; Regularisation Noem Best subset &amp; stepwise, maar geen detail 3.1 L1 and L2 Bring teorie in vir \\(\\hat{\\beta_{RIDGE}}\\) en Lasso. Kyk na Othonormal covariates op Lasso wiki 3.2 ElasticNet 3.3 PCR &amp; PLS Sal sien oor die PLS, hang af van die tyd en scope.Versigtig vir ISLR, nie noodwendig reg nie. A3Q2 "],["classification-models.html", "Chapter 4 Classification Models 4.1 Logistic regression 4.2 Model evaluation", " Chapter 4 Classification Models 4.1 Logistic regression 4.2 Model evaluation 4.2.1 ROC Curves 4.2.2 PR curves "],["beyond-linearity.html", "Chapter 5 Beyond Linearity 5.1 Polynomial 5.2 GAMs 5.3 WLS - LOESS/LOWESS??? 5.4 KNN", " Chapter 5 Beyond Linearity 5.1 Polynomial Both for linear regression model extension and logistic regression 5.2 GAMs A3Q3? 5.3 WLS - LOESS/LOWESS??? Well, I like it… 5.4 KNN "],["tree-based-methods.html", "Chapter 6 Tree-based Methods 6.1 CART 6.2 Bagging &amp; RF 6.3 Boosting", " Chapter 6 Tree-based Methods 6.1 CART Murphy het ook ’n nice verduideliking hieroor in 16.2. Iris is dalk ook ’n goeie illustrative voorbeeld. 6.2 Bagging &amp; RF 6.3 Boosting Gebruik Murphy 16.4 as die basis hiervoor Vir application gebruik caret en tidymodels (met tune, fine, workflowsets). Daar is ook nuwe goed soos hierdie https://arxiv.org/abs/2303.12177 "],["references.html", "References", " References Abu-Mostafa, Y. S., Magdon-Ismail, M., &amp; Lin, H.-T. (2012). Learning from data. AMLBook. Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: Data mining, inference, and prediction (Vol. 2). Springer. James, G., Witten, D., Hastie, T., Tibshirani, R., et al. (2013). An introduction to statistical learning (Vol. 112). Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
