<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Supervised Learning |  STA4026S – Honours Analytics Section B: Theory and Application of Supervised Learning</title>
  <meta name="description" content="<br />
STA4026S – Honours Analytics<br />
Section B: Theory and Application of Supervised Learning</p>" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Supervised Learning |  STA4026S – Honours Analytics Section B: Theory and Application of Supervised Learning" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Supervised Learning |  STA4026S – Honours Analytics Section B: Theory and Application of Supervised Learning" />
  
  
  

<meta name="author" content="Stefan S. Britz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="model-selection-regularisation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.1</b> Bias-Variance trade-off</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#example-1-simulation-1"><i class="fa fa-check"></i><b>2.1.1</b> Example 1 – Simulation 1</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#model-validation"><i class="fa fa-check"></i><b>2.2</b> Model validation</a></li>
<li class="chapter" data-level="2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#side-note-statistical-learning-vs-machine-learning"><i class="fa fa-check"></i><b>2.3</b> Side note: Statistical learning vs machine learning</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="model-selection-regularisation.html"><a href="model-selection-regularisation.html"><i class="fa fa-check"></i><b>3</b> Model Selection &amp; Regularisation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="model-selection-regularisation.html"><a href="model-selection-regularisation.html#l1-and-l2"><i class="fa fa-check"></i><b>3.1</b> L1 and L2</a></li>
<li class="chapter" data-level="3.2" data-path="model-selection-regularisation.html"><a href="model-selection-regularisation.html#elasticnet"><i class="fa fa-check"></i><b>3.2</b> ElasticNet</a></li>
<li class="chapter" data-level="3.3" data-path="model-selection-regularisation.html"><a href="model-selection-regularisation.html#pcr-pls"><i class="fa fa-check"></i><b>3.3</b> PCR &amp; PLS</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-models.html"><a href="classification-models.html"><i class="fa fa-check"></i><b>4</b> Classification Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-models.html"><a href="classification-models.html#logistic-regression"><i class="fa fa-check"></i><b>4.1</b> Logistic regression</a></li>
<li class="chapter" data-level="4.2" data-path="classification-models.html"><a href="classification-models.html#model-evaluation"><i class="fa fa-check"></i><b>4.2</b> Model evaluation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-models.html"><a href="classification-models.html#roc-curves"><i class="fa fa-check"></i><b>4.2.1</b> ROC Curves</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-models.html"><a href="classification-models.html#pr-curves"><i class="fa fa-check"></i><b>4.2.2</b> PR curves</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="beyond-linearity.html"><a href="beyond-linearity.html"><i class="fa fa-check"></i><b>5</b> Beyond Linearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="beyond-linearity.html"><a href="beyond-linearity.html#gams"><i class="fa fa-check"></i><b>5.1</b> GAMs</a></li>
<li class="chapter" data-level="5.2" data-path="beyond-linearity.html"><a href="beyond-linearity.html#wls---loesslowess"><i class="fa fa-check"></i><b>5.2</b> WLS - LOESS/LOWESS???</a></li>
<li class="chapter" data-level="5.3" data-path="beyond-linearity.html"><a href="beyond-linearity.html#knn"><i class="fa fa-check"></i><b>5.3</b> KNN</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>6</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#cart"><i class="fa fa-check"></i><b>6.1</b> CART</a></li>
<li class="chapter" data-level="6.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-rf"><i class="fa fa-check"></i><b>6.2</b> Bagging &amp; RF</a></li>
<li class="chapter" data-level="6.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>6.3</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p><img src="figs/Dept_Logo.jpg" style="width:50.0%" /><img src="figs/UCTLogo.jpg" style="width:20.0%" /><br />
STA4026S – Honours Analytics<br />
Section B: Theory and Application of Supervised Learning</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Supervised Learning<a href="supervised-learning.html#supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, most of which you should be familiar with already. Whereas there are many good sources that provide a more comprehensive discussion, Chapter 2 of <span class="citation">James et al. (2013)</span> is sufficient for the level and scope of this course.</p>
<p>Using a set of observations to uncover some underlying process in the real world is the basic premise of “learning from data” <span class="citation">(Abu-Mostafa et al., 2012, p. 11)</span>. By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course.</p>
<p>The basic distinction is that <strong>supervised learning</strong> refers to cases where there is some <strong>target variable</strong>, usually indicated as <span class="math inline">\(Y\)</span>, whereas if we are interested in the structures and patterns in explanatory variables<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> only, we refer to unsupervised learning.</p>
<p>Given a quantitative response <span class="math inline">\(Y\)</span> and a set of <span class="math inline">\(p\)</span> predictors <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>, we are interested in the assumed, unobserved function that maps the inputs to the outputs:</p>
<p><span class="math display">\[Y = \underbrace{f(X)}_{systematic} + \underbrace{\epsilon}_{random},\]</span></p>
<p>where <span class="math inline">\(f(.)\)</span> represents the fixed, but unknown function and <span class="math inline">\(\epsilon\)</span> is a random error term, independent of <span class="math inline">\(X\)</span>, with <span class="math inline">\(E(X) = 0\)</span>. By estimating <span class="math inline">\(f\)</span> such that</p>
<p><span class="math display">\[\hat{Y} = \hat{f}(X),\]</span></p>
<p>we allow for both prediction of <span class="math inline">\(Y\)</span> – which is the primary goal in forecasting – and inference, i.e. describing how <span class="math inline">\(Y\)</span> is affected by changes in <span class="math inline">\(X\)</span>.</p>
<p>Hypothesising <span class="math inline">\(\hat{f}\)</span> can be done in two ways, namely via a parametric or a non-parametric approach.</p>
<p><strong>Parametric approach</strong></p>
<p>Here an assumption is made about the the functional form of <span class="math inline">\(f\)</span>, for example</p>
<p><span class="math display">\[f(\boldsymbol{X}) = \beta_0 + \sum_{j=1}^p\beta_jX_j.\]</span></p>
<p>The best estimate of <span class="math inline">\(f\)</span> is now defined as the set of parameters <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimise some specified loss function. Given a set of observations <span class="math inline">\(\mathcal{D}=\{\boldsymbol{x}_i, y_i\}_{i=1}^n\)</span> – henceforth referred to as the <strong>training set</strong> – we could use ordinary least squares to minimise the mean squared error (MSE):</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^n\left[y_i - \hat{f}(x_i)\right]^2 \]</span> Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters.</p>
<p><strong>Non-parametric approach</strong></p>
<p>Another option is to make no explicit assumptions regarding the functional form of <span class="math inline">\(f\)</span>. This allows one to fit a wide range of possible forms for <span class="math inline">\(f\)</span> – in these notes we consider K-nearest neighbours and tree-based methods – but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation.</p>
<p>The objective remains to find <span class="math inline">\(f\)</span> that fits the available data as closely as possible, whilst avoiding overfitting to ensure that the model generalises well to unseen data.</p>
<p><strong>Generalisation</strong></p>
<p>The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model, referred to as out-of-sample observations.</p>
<p>Consider the case where the estimated function <span class="math inline">\(\hat{f}\)</span> is fixed and out-of-sample observations of the variables are introduced, which we will denote as <span class="math inline">\(\{\boldsymbol{x}_0, y_0\}\)</span>. The expected MSE for these <strong>test set</strong> observations (see Section <a href="supervised-learning.html#model-validation">2.2</a>) can be deconstructed as follows:</p>
<span class="math display" id="eq:test-mse-decomp">\[\begin{align}
E\left[y_0 - \hat{f}(\boldsymbol{x}_0) \right]^2 &amp;= E\left[f(\boldsymbol{x}_0) + \epsilon - \hat{f}(\boldsymbol{x}_0)\right]^2 \\
&amp;= E\left[\left(f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right)^2 + 2\epsilon \left(f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0) \right) + \epsilon^2\right] \\
&amp;= E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right]^2 + 2E[\epsilon]E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right] + E\left[\epsilon^2\right] \\
&amp;= \underbrace{E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right]^2}_{reducible} + \underbrace{Var(\epsilon)}_{irreducible} \tag{2.1}
\end{align}\]</span>
<p>The primary goal of machine learning is to find an <span class="math inline">\(\hat{f}\)</span> that best approximates the underlying, unknown relationship between the input and output variables by minimising the reducible error. Note that because of the irreducible component (the “noise” in the data), there will always be some lower bound for the theoretical MSE, and that <strong>this bound is almost always unknown in practice</strong> <span class="citation">(James et al., 2013, p. 19)</span>.</p>
<p>By now you will be familiar with the concept of bias-variance trade-off, according to which we attempt to find a sufficiently (but not overly) complex model. The reducible error component can be decomposed further to help illustrate this trade-off.</p>
<div id="bias-variance-trade-off" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Bias-Variance trade-off<a href="supervised-learning.html#bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider again a fixed <span class="math inline">\(\hat{f}\)</span> and out-of-sample observations <span class="math inline">\(\{\boldsymbol{x}_0, y_0\}\)</span>. For ease of notation, let <span class="math inline">\(f = f(\boldsymbol{x}_0)\)</span> and <span class="math inline">\(\hat{f} = \hat{f}(\boldsymbol{x}_0)\)</span>. Also note that <span class="math inline">\(f\)</span> is deterministic such that <span class="math inline">\(E\left[f\right] = f\)</span>.</p>
<p>Starting with the reducible error in Equation <a href="supervised-learning.html#eq:test-mse-decomp">(2.1)</a>, we have</p>
<span class="math display" id="eq:bias-var">\[\begin{align}
E\left[f - \hat{f} \right]^2 &amp;= E\left[\hat{f} - f \right]^2 \\
&amp;= E\left[\hat{f} - E(\hat{f}) + E(\hat{f}) - f \right]^2 \\
&amp;= E\left\{ \left[\hat{f} - E(\hat{f})\right]^2 + 2\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right] + \left[E(\hat{f}) - f\right]^2 \right\} \\
&amp;= E\left[\hat{f} - E(\hat{f})\right]^2 + 2E\left\{\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right]\right\} + E\left[E(\hat{f}) - f\right]^2 \\
&amp;= Var\left[\hat{f}\right] + 0 + \left[E(\hat{f}) - f\right]^2 \\
&amp;= Var\left[\hat{f}\right] + Bias^2\left[\hat{f}\right] \tag{2.2}
\end{align}\]</span>
<p>Showing that the crossproduct term equals zero:</p>
<span class="math display">\[\begin{align}
E\left\{\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right]\right\} &amp;= E\left[\hat{f}E(\hat{f}) - E(\hat{f})E(\hat{f}) - \hat{f}f + E(\hat{f})f\right] \\
&amp;= E(\hat{f})E(\hat{f}) - E(\hat{f})E(\hat{f}) - E(\hat{f})f + E(\hat{f})f \\
&amp;= 0
\end{align}\]</span>
<p>Therefore, in order to minimise the expected test MSE we need to find a model that has the lowest combined variance and (squared) bias.</p>
<p>The <strong>variance</strong> represents the extent to which <span class="math inline">\(\hat{f}\)</span> changes between different training samples taken from the same population. The bias of <span class="math inline">\(\hat{f}\)</span> is simply the error that is introduced by approximating the real-world relationship with a simpler representation. Note, however, that since <span class="math inline">\(f\)</span> is generally unknown, the bias component cannot be directly observed or measured outside of simulations. However, these simulations may help us illustrate how the bias and variance change as model complexity increases.</p>
<p>Although the concepts of model complexity and flexibility are not necessarily perfectly defined – depending on the class of model being hypothesised – the following example should provide an intuitive understanding.</p>
<div id="example-1-simulation-1" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Example 1 – Simulation 1<a href="supervised-learning.html#example-1-simulation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="model-validation" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Model validation<a href="supervised-learning.html#model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Hoef nie CV van scratch af nie, Et het die beginsel gedek.</p>
<p>This has been a very brief summary of some of the basic statistical learning principles we will encounter going forward. For a more in-depth discussion,</p>
</div>
<div id="side-note-statistical-learning-vs-machine-learning" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Side note: Statistical learning vs machine learning<a href="supervised-learning.html#side-note-statistical-learning-vs-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Statistical Learning:</p>
<p>Foundation: Statistical learning is rooted in the field of statistics and builds upon principles of probability, inference, and hypothesis testing. Objective: The primary objective of statistical learning is to understand and model relationships between variables in order to make predictions or draw insights from data. Emphasis: Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. Assumptions: It often involves making explicit assumptions about the underlying statistical distributions and relationships between variables. Interpretability: Statistical models tend to be more interpretable, as they often involve understanding the impact and significance of individual variables on the outcome.</p>
<p>Foundation: Machine learning draws from various fields, including computer science, artificial intelligence, and statistical learning, but with a stronger emphasis on algorithmic and computational aspects. Objective: The primary objective of machine learning is to develop algorithms that can automatically learn patterns and relationships from data, with an emphasis on predictive accuracy and generalization to new, unseen data. Emphasis: While understanding the underlying data-generating process is important, machine learning is often more focused on achieving optimal predictive performance. Assumptions: Machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions. Interpretability: Some machine learning models, like deep neural networks, can be highly complex and challenging to interpret due to their architecture and the multitude of learned features.</p>
<p>It’s important to note that the distinction between statistical learning and machine learning can sometimes be blurred, and the two fields have evolved to influence each other over time. Many modern machine learning techniques, such as ensemble methods and regularization, have strong roots in statistical learning theory. Additionally, both fields share common goals of extracting insights from data and making predictions, but they may approach these goals with different philosophical and methodological perspectives.</p>
<p>In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Often referred to as <strong>features</strong> in the learning context, or <strong>predictors</strong> in supervised learning specifically.<a href="supervised-learning.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-selection-regularisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SSBritz/STA4026S-Analytics-SecB/edit/main/02-SL.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/SSBritz/STA4026S-Analytics-SecB/blob/main/02-SL.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
