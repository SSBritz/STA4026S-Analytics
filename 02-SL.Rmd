---
csl: apa.csl
---

# Supervised Learning

This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, most of which you should be familiar with already. Whereas there are many good sources that provide a more comprehensive discussion, Chapter 2 of @james2013introduction is sufficient for the level and scope of this course. 

Using a set of observations to uncover some underlying process in the real world is the basic premise of "learning from data" [@Mostafa2012, p. 11]. By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course.

The basic distinction is that **supervised learning** refers to cases where there is some **target variable**, usually indicated as $Y$, whereas if we are interested in the structures and patterns in explanatory variables[^02-sl-1] only, we refer to unsupervised learning.

[^02-sl-1]: Often referred to as **features** in the learning context, or **predictors** in supervised learning specifically.

Given a quantitative response $Y$ and a set of $p$ predictors $X_1, X_2, \ldots, X_p$, we are interested in the assumed, unobserved function that maps the inputs to the outputs:

$$Y = \underbrace{f(X)}_{systematic} + \underbrace{\epsilon}_{random},$$

where $f(.)$ represents the fixed, but unknown function and $\epsilon$ is a random error term, independent of $X$, with $E(X) = 0$. By estimating $f$ such that

$$\hat{Y} = \hat{f}(X),$$

we allow for both prediction of $Y$ -- which is the primary goal in forecasting -- and inference, i.e. describing how $Y$ is affected by changes in $X$.

Hypothesising $\hat{f}$ can be done in two ways, namely via a parametric or a non-parametric approach.

**Parametric approach**

Here an assumption is made about the the functional form of $f$, for example

$$f(\boldsymbol{X}) = \beta_0 + \sum_{j=1}^p\beta_jX_j.$$

The best estimate of $f$ is now defined as the set of parameters $\hat{\boldsymbol{\beta}}$ that minimise some specified loss function. Given a set of observations $\mathcal{D}=\{\boldsymbol{x}_i, y_i\}_{i=1}^n$ -- henceforth referred to as the **training set** -- we could use ordinary least squares to minimise the mean squared error (MSE):

$$MSE = \frac{1}{n}\sum_{i=1}^n\left[y_i - \hat{f}(x_i)\right]^2 $$ Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters.

**Non-parametric approach**

Another option is to make no explicit assumptions regarding the functional form of $f$. This allows one to fit a wide range of possible forms for $f$ -- in these notes we consider K-nearest neighbours and tree-based methods -- but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation.

The objective remains to find $f$ that fits the available data as closely as possible, whilst avoiding overfitting to ensure that the model generalises well to unseen data.

**Generalisation**

The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model, referred to as out-of-sample observations.

Consider the case where the estimated function $\hat{f}$ is fixed and out-of-sample observations of the variables are introduced, which we will denote as $\{\boldsymbol{x}_0, y_0\}$. The expected MSE for these **test set** observations (see Section \@ref(model-validation)) can be deconstructed as follows:

```{=tex}
\begin{align}
E\left[y_0 - \hat{f}(\boldsymbol{x}_0) \right]^2 &= E\left[f(\boldsymbol{x}_0) + \epsilon - \hat{f}(\boldsymbol{x}_0)\right]^2 \\
&= E\left[\left(f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right)^2 + 2\epsilon \left(f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0) \right) + \epsilon^2\right] \\
&= E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right]^2 + 2E[\epsilon]E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right] + E\left[\epsilon^2\right] \\
&= \underbrace{E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right]^2}_{reducible} + \underbrace{Var(\epsilon)}_{irreducible} (\#eq:test-mse-decomp)
\end{align}
```
The primary goal of machine learning is to find an $\hat{f}$ that best approximates the underlying, unknown relationship between the input and output variables by minimising the reducible error. Note that because of the irreducible component (the "noise" in the data), there will always be some lower bound for the theoretical MSE, and that **this bound is almost always unknown in practice** [@james2013introduction, p. 19].

By now you will be familiar with the concept of bias-variance trade-off, according to which we attempt to find a sufficiently (but not overly) complex model. The reducible error component can be decomposed further to help illustrate this trade-off.

## Bias-Variance trade-off

Consider again a fixed $\hat{f}$ and out-of-sample observations $\{\boldsymbol{x}_0, y_0\}$. For ease of notation, let $f = f(\boldsymbol{x}_0)$ and $\hat{f} = \hat{f}(\boldsymbol{x}_0)$. Also note that $f$ is deterministic such that $E\left[f\right] = f$.

Starting with the reducible error in Equation \@ref(eq:test-mse-decomp), we have

```{=tex}
\begin{align}
E\left[f - \hat{f} \right]^2 &= E\left[\hat{f} - f \right]^2 \\
&= E\left[\hat{f} - E(\hat{f}) + E(\hat{f}) - f \right]^2 \\
&= E\left\{ \left[\hat{f} - E(\hat{f})\right]^2 + 2\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right] + \left[E(\hat{f}) - f\right]^2 \right\} \\
&= E\left[\hat{f} - E(\hat{f})\right]^2 + 2E\left\{\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right]\right\} + E\left[E(\hat{f}) - f\right]^2 \\
&= Var\left[\hat{f}\right] + 0 + \left[E(\hat{f}) - f\right]^2 \\
&= Var\left[\hat{f}\right] + Bias^2\left[\hat{f}\right] (\#eq:bias-var)
\end{align}
```
Showing that the crossproduct term equals zero:

```{=tex}
\begin{align}
E\left\{\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right]\right\} &= E\left[\hat{f}E(\hat{f}) - E(\hat{f})E(\hat{f}) - \hat{f}f + E(\hat{f})f\right] \\
&= E(\hat{f})E(\hat{f}) - E(\hat{f})E(\hat{f}) - E(\hat{f})f + E(\hat{f})f \\
&= 0
\end{align}
```
Therefore, in order to minimise the expected test MSE we need to find a model that has the lowest combined variance and (squared) bias.

The **variance** represents the extent to which $\hat{f}$ changes between different training samples taken from the same population. The bias of $\hat{f}$ is simply the error that is introduced by approximating the real-world relationship with a simpler representation. Note, however, that since $f$ is generally unknown, the bias component cannot be directly observed or measured outside of simulations. However, these simulations may help us illustrate how the bias and variance change as model complexity increases.

Although the concepts of model complexity and flexibility are not necessarily perfectly defined -- depending on the class of model being hypothesised -- the following example should provide an intuitive understanding.

### Example 1 -- Simulation 1

To allow for easy visualisation, let us consider a simple model with only one feature:

$$Y = X + 2\cos(5X) + \epsilon,$$
where $\epsilon \sim N(0, 2)$.

Below we simulate $n = 100$ observations from $X \sim U(-2,2)$, to which we fit cubic smoothing splines of increasing complexity. [Splines are beyond the scope of this course, but provide an easy-to-see illustration of "flexibility"]

```{r, animation.hook='gifski', interval=0.333}
set.seed(4026)

#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))

#The true function
xx <- seq(-2, 2, length.out = 1000)
f <- xx + 2*cos(5*xx)

#Fit cubic splines with increasing degrees of freedom
for(dof in 2:50){
  fhat <- smooth.spline(x, y, df = dof)
  plot(x, y, pch = 16)
  lines(xx, f, 'l', lwd = 2)
  lines(fhat, col = 'blue', lwd = 2)
  title(main = paste('Degrees of freedom:', dof))
  legend('bottomright', c('f(x) - True', expression(hat(f)(x) ~ '- Cubic spline')), 
         col = c('black', 'blue'), lty = 1, lwd = 2)
}
```

This serves to illustrate how the model's degrees of freedom is directly proportional to the model's complexity. However, to extricate the bias and variance components, we need to observe these models' fit on out-of-sample data across many random realisations of training samples.  

In the following simulation we again observe $n=100$ training observations at a time, to which the same models of varying complexity as above are fitted. Each model's fit is then assessed on a fixed set of $100$ testing observations. This process is repeated $250$ times, such that we can keep track of how each test observation's predictions vary across the iterations, as well as the errors. 

```{r}
set.seed(1)

n <- 100        #Sample size
num_sims <- 1000#Number of iterations
dofs <- 2:24    #Model complexities

pred_mat <- matrix(nrow = num_sims, ncol = n) #To store each set of predictions
mses <- vector(length = num_sims)     #Also want to track the testing MSEs
red_err <- vector(length = num_sims)  #As well as the reducible error

#Herein we will capture the deconstructed components for each model
results <- data.frame(Var = NA, Bias2 = NA, Red_err = NA, MSE = NA)

#Testing data
x_test <- runif(n, -2, 2)
f_test <- x_test + 2*cos(5*x_test) #This is the part we don't know outside sims!!
y_test <- f_test + rnorm(n, sd = sqrt(2))

d <- 0 #To keep track of dof iterations, whilst easily changing the range above

for(dof in dofs) { #Repeat over all model complexities
  d <- d+1
  for(iter in 1:num_sims){ 
    
    #Training data
    x_train <- runif(n, -2, 2)
    y_train <- x_train + 2*cos(5*x_train) + rnorm(n, sd = sqrt(2))
    
    #Fit cubic spline
    spline_mod <- smooth.spline(x_train, y_train, df = dof)
    
    #Predict on OOS data
    yhat <- predict(spline_mod, x_test)$y
    
    #And store
    pred_mat[iter, ] <- yhat
    red_err[iter] <- mean((f_test - yhat)^2)
    mses[iter] <- mean((y_test - yhat)^2)
  }
  
  var_fhat <- mean(apply(pred_mat, 2, var)) #E[\hat{f} - E(\hat{f})]^2
  bias2_fhat <- mean((colMeans(pred_mat) - f_test)^2) #[E(\hat{f}) - f]^2
  reducible <- mean(red_err) #E[f - \hat{f}]^2
  MSE <- mean(mses)          #E[y_0 - \hat{f}]^2
  
  results[d, ] <- c(var_fhat, bias2_fhat, reducible, MSE)
}

#Plot the results

plot(dofs, results$Var, 'l', col = 'orange', lwd = 2,
     xlab = 'Model complexity', ylab = '', ylim = c(0, max(results[,-4])))
lines(dofs, results$Bias2, 'l', col = 'lightblue', lwd = 2)
lines(dofs, results$Red_err, 'l', col = 'darkred', lwd = 2)
abline(v = dofs[which.min(results$Red_err)], lty = 2)
# lines(dofs, results[, 4], 'l', lwd = 2)
```



## Model validation

Hoef nie CV van scratch af nie, Et het die beginsel gedek.

This has been a very brief summary of some of the basic statistical learning principles we will encounter going forward. For a more in-depth discussion,

## Side note: Statistical learning vs machine learning

Statistical Learning:

Foundation: Statistical learning is rooted in the field of statistics and builds upon principles of probability, inference, and hypothesis testing. Objective: The primary objective of statistical learning is to understand and model relationships between variables in order to make predictions or draw insights from data. Emphasis: Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. Assumptions: It often involves making explicit assumptions about the underlying statistical distributions and relationships between variables. Interpretability: Statistical models tend to be more interpretable, as they often involve understanding the impact and significance of individual variables on the outcome.

Foundation: Machine learning draws from various fields, including computer science, artificial intelligence, and statistical learning, but with a stronger emphasis on algorithmic and computational aspects. Objective: The primary objective of machine learning is to develop algorithms that can automatically learn patterns and relationships from data, with an emphasis on predictive accuracy and generalization to new, unseen data. Emphasis: While understanding the underlying data-generating process is important, machine learning is often more focused on achieving optimal predictive performance. Assumptions: Machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions. Interpretability: Some machine learning models, like deep neural networks, can be highly complex and challenging to interpret due to their architecture and the multitude of learned features.

It's important to note that the distinction between statistical learning and machine learning can sometimes be blurred, and the two fields have evolved to influence each other over time. Many modern machine learning techniques, such as ensemble methods and regularization, have strong roots in statistical learning theory. Additionally, both fields share common goals of extracting insights from data and making predictions, but they may approach these goals with different philosophical and methodological perspectives.

In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance.
