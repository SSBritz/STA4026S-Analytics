valid_ind <- ind[(1+10*(k-1)):10*k]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
pred <- predict(fit, x[valid_ind])
test <- pred$y
cv_k <- mean((test - y[valid_ind])^2)
pred$x
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
?lines
?segments
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], 0)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
CV_k <- vector(length = 10)
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[(1+10*(k-1)):10*k]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
cv_k <- mean((test - y[valid_ind])^2)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
}
k <- 1
for(k in 1){
valid_ind <- ind[(1+10*(k-1)):10*k]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
cv_k <- mean((test - y[valid_ind])^2)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
}
for(k in 1:2){
valid_ind <- ind[(1+10*(k-1)):10*k]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
cv_k <- mean((test - y[valid_ind])^2)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
}
for(k in 1:9){
valid_ind <- ind[(1+10*(k-1)):10*k]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
cv_k <- mean((test - y[valid_ind])^2)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
}
ind
ind[(1+10*(k-1)):10*k]
10*k
(1+10*(k-1))
(1+10*(k-1)):10*k
ind(31:40)
ind[31:40]
valid_ind <- ind[[(1+10*(k-1)):10*k]]
valid_ind <- ind[(1+10*(k-1)):10*k]
(1+10*(k-1)):10*k
valid_ind <- ind[1+10*(k-1):10*k]
valid_ind <- ind[((1+10*(k-1))):10*k]
ai <-
valid_ind <- ind[]
ai <- (1+10*(k-1)):10*k
(1+10*(k-1))
seq((1+10*(k-1)), 10*k)
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
CV_k <- vector(length = 10)
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
cv_k <- mean((test - y[valid_ind])^2)
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y')
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
}
train_fit <- predict(fit, x[-valid_ind])$y
train_err <- mean((train_fit - y[-valid_ind])^2)
cv_k <- c()
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(0, 10))
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16,
xlab = 'x', ylab = 'y', ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(0, 10))
}
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16,
xlab = 'x', ylab = 'y', ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(1, 10))
lines(1:k, train_err, 'b')
}
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16,
xlab = 'x', ylab = 'y', ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(1, 10))
lines(1:k, train_err, 'b', lwd = 2)
}
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16,
xlab = 'x', ylab = 'y', ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(1, 10))
lines(1:k, train_err, 'b', lwd = 2)
legend('topright', c('Training error', 'Validation error'),
col = c('black', 'red'))
}
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16,
xlab = 'x', ylab = 'y', ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(1, 10))
lines(1:k, train_err, 'b', lwd = 2)
legend('topright', c('Training error', 'Validation error'),
col = c('black', 'red'), lwd = 2)
}
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16,
xlab = 'x', ylab = 'y', ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
lines(fit, col = 'blue', lwd = 2)
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright',
c('Training', 'Validation', expression(hat(f)(x)), 'Validation errors'),
col = c('black', 'gray', 'blue', 'red'),
pch = c(16, 16, NA, NA),
lty = c(NA, NA, 1, 3),
lwd = c(NA, NA, 2, 2))
plot(1:k, cv_k, 'b', col = 'red', lwd = 2,
xlab = 'Fold', ylab = 'Error', xlim = c(1, 10), ylim = c(1, 5.5))
lines(1:k, train_err, 'b', lwd = 2)
legend('topright', c('Training error', 'Validation error'),
col = c('black', 'red'), lwd = 2)
}
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
rm(list = ls())
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
#The true function
xx <- seq(-2, 2, length.out = 1000)
f <- xx + 2*cos(5*xx)
#Fit cubic splines with increasing degrees of freedom
for(dof in 2:50){
fhat <- smooth.spline(x, y, df = dof)
plot(x, y, pch = 16)
lines(xx, f, 'l', lwd = 2)
lines(fhat, col = 'blue', lwd = 2)
title(main = paste('Degrees of freedom:', dof))
legend('bottomright', c('f(x) - True', expression(hat(f)(x) ~ '- Cubic spline')),
col = c('black', 'blue'), lty = 1, lwd = 2)
}
set.seed(1)
n <- 100          #Sample size
num_sims <- 1000  #Number of iterations (could be parallelised)
dofs <- 2:25      #Model complexities
var_eps <- 2      #Var(epsilon): The irreducible error
pred_mat <- matrix(nrow = num_sims, ncol = n) #To store each set of predictions
mses <- vector(length = num_sims)             #Also want to track the testing MSEs
red_err <- vector(length = num_sims)          #As well as the reducible error
#Herein we will capture the deconstructed components for each model
results <- data.frame(Var = NA, Bias2 = NA, Red_err = NA, MSE = NA)
#Testing data
x_test <- runif(n, -2, 2)
f_test <- x_test + 2*cos(5*x_test) #This is the part we don't know outside sims!!
d <- 0 #To keep track of dof iterations, even when changing the range
for(dof in dofs) { #Repeat over all model complexities
d <- d+1
for(iter in 1:num_sims){
#Training data
x_train <- runif(n, -2, 2)
y_train <- x_train + 2*cos(5*x_train) + rnorm(n, sd = sqrt(var_eps))
#Add the noise
y_test <- f_test + rnorm(n, sd = sqrt(var_eps))
#Fit cubic spline
spline_mod <- smooth.spline(x_train, y_train, df = dof)
#Predict on OOS data
yhat <- predict(spline_mod, x_test)$y
#And store
pred_mat[iter, ] <- yhat
red_err[iter] <- mean((f_test - yhat)^2)
mses[iter] <- mean((y_test - yhat)^2)
}
#Average each component over all iterations
var_fhat <- mean(apply(pred_mat, 2, var))           #E[\hat{f} - E(\hat{f})]^2
bias2_fhat <- mean((colMeans(pred_mat) - f_test)^2) #E[E(\hat{f}) - f]^2
reducible <- mean(red_err)                          #E[f - \hat{f}]^2
MSE <- mean(mses)                                   #E[y_0 - \hat{f}]^2
results[d, ] <- c(var_fhat, bias2_fhat, reducible, MSE)
}
#Plot the results
plot(dofs, results$MSE, 'l', col = 'darkred', lwd = 2,
xlab = 'Model complexity', ylab = '', ylim = c(0, max(results)))
lines(dofs, results$Bias2, 'l', col = 'lightblue', lwd = 2)
lines(dofs, results$Var, 'l', col = 'orange', lwd = 2)
lines(dofs, results$Red_err, 'l', lty = 2, lwd = 2)
legend('topright',
c('MSE', expression(Bias^2 ~ (hat(f))), expression(Var(hat(f))), 'Reducible Error'),
col = c('darkred', 'lightblue', 'orange', 'black'), lty = c(rep(1, 3), 2), lwd = 2)
abline(v = dofs[which.min(results$MSE)], lty = 3) #Complexity minimising MSE
abline(h = var_eps, lty = 3)                      #MSE lower bound
#Is reducible error = var(fhat) + bias^2(fhat)?
ifelse(isTRUE(all.equal(results$Red_err,
results$Var + results$Bias2,
tolerance = 0.001)),
'Happy days! :D',
'Haibo...')
#Is Test MSE = var(fhat) + bias^2(fhat) + var(eps)?
ifelse(isTRUE(all.equal(results$MSE,
results$Var + results$Bias2 + var_eps,
tolerance = 0.01)),
'Happy days! :D',
'Haibo...')
set.seed(4026)
#Simulated data
x <- runif(100, -2, 2)
y <- x + 2*cos(5*x) + rnorm(100, sd = sqrt(2))
cv_k <- c()
train_err <- c()
#10-fold CV
ind <- sample(1:100) #Don't actually need to randomise here, but should in general
for(k in 1:10){
valid_ind <- ind[seq((1+10*(k-1)), 10*k)]
fit <- smooth.spline(x[-valid_ind], y[-valid_ind], df = 8)
test <- predict(fit, x[valid_ind])$y
train_fit <- predict(fit, x[-valid_ind])$y
cv_k <- c(cv_k, mean((test - y[valid_ind])^2))
train_err <- c(train_err, mean((train_fit - y[-valid_ind])^2))
#Should write the above into a function...
#But the plotting needs to be inside the loop for the rendering
par(mfrow = c(1, 2))
plot(x[-valid_ind], y[-valid_ind], pch = 16, xlab = 'x', ylab = 'y',
xlim = c(min(x), max(x)), ylim = c(min(y), max(y)))
points(x[valid_ind], y[valid_ind], pch = 16, col = 'gray')
segments(x[valid_ind], y[valid_ind], x[valid_ind], test,
col = 'red', lty = 3, lwd = 2)
lines(fit, col = 'blue', lwd = 2)
title(main = paste('Fold:', k))
legend('bottomright', c('Training', 'Validation', 'Errors'),
col = c('black', 'gray', 'red'),
pch = c(16, 16, NA),
lty = c(NA, NA, 3),
lwd = c(NA, NA, 2))
plot(1:k, cv_k, 'b', pch = 16, col = 'red', lwd = 2, xlab = 'Fold', ylab = 'MSE',
xlim = c(1, 10), ylim = c(1, 5.5))
lines(1:k, train_err, 'b', pch = 16, lwd = 2)
legend('topright', c('Training', 'Validation'), col = c('black', 'red'), lwd = 2)
}
#Keep track of MSE per fold, per model
fold_mses <- matrix(nrow = 10, ncol = length(dofs))
for(k in 1:10){
cv_ind <- ind[seq((1+10*(k-1)), 10*k)]
d <- 0
for(dof in dofs){ #Using the same dofs as earlier
d <- d + 1
fit <- smooth.spline(x[-cv_ind], y[-cv_ind], df = dof)
test <- predict(fit, x[cv_ind])$y
fold_mses[k, d] <- mean((test - y[cv_ind])^2)
}
}
#Average over all folds
cv_mses <- colMeans(fold_mses)
# Compare the true MSE from earlier
plot(dofs, results$MSE, 'l', col = 'darkred', lwd = 2,
xlab = 'Model complexity', ylab = '', ylim = c(0, max(cv_mses)))
lines(dofs, cv_mses, 'l', col = 'grey', lwd = 2)
legend('bottomright', c('CV MSE', 'True test MSE'), col = c('gray', 'darkred'), lty = 1, lwd = 2)
abline(v = dofs[which.min(results$MSE)], lty = 3)
points(dofs[which.min(cv_mses)], min(cv_mses), pch = 13, cex = 2.5, lwd = 2)
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
