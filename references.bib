@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and others},
  volume={112},
  year={2013},
  publisher={Springer}
}

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}

@book{Mostafa2012,
author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
title = {Learning From Data},
year = {2012},
isbn = {1600490069},
publisher = {AMLBook},
}

@article{elasticnet,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p â‰« n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 urldate = {2023-08-23},
 volume = {67},
 year = {2005}
}
